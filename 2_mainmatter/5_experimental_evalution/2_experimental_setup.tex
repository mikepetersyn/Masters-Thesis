\documentclass[../../main.tex]{subfiles}
\begin{document}

\section{Experimental Setup}

Section~\ref{subsec:implementation_details} presents details on the implementation of the application. This includes the conceptual realization of the architecture that is outlined in Section~\ref{sec:architecture_specification} and the technology solutions that were adopted.  Furthermore, internals of the data and communication flow between individual services of the application are explained. Finally, two aspects are discussed in more detail. For one thing, the technology for parallelization and thus scaling is covered. For another, the differentiated possibilities for data consistency will be highlighted. Section~\ref{subsec:methodologies} discusses the design of the experiments used to evaluate the advantages of the \gls{cids} compared to isolated \gls{ids}. The focus here is on testing the improvement of the detection performance under the assumption of the existence of local knowledge gaps and the reduction of the data volume to be exchanged.

\subsection{Implementation Details}\label{subsec:implementation_details}

A cloud-oriented architecture, consisting of four services for data processing (see Section~\ref{sec:architecture_specification}), is realized as a python streaming application \footnote{\url{https://gitlab.informatik.hs-fulda.de/verca/pattern-database}}. The data processing services, message queues and key-value stores are deployed using docker compose. Scalability, availability and resilience are achieved by deploying each service in a replicated fashion. Kafka is employed as the message queue that realizes the global and local event channels, that are responsible for distributing jobs among instances of the processing services. The global and local pattern databases are realized using Redis. As described in Section~\ref{sec:high_level_overview}, intrusion detection datasets are partitioned into regions and stored by using the respective regions as keys. In that way, data parallelization is achieved. In particular, data from each region in the key-value store can be processed independently. Hence, regions are also exploited as a job-distribution primitive. In other words, each processing service receives regions by listening on a specific topic of a Kafka message broker. As all instances of a service belong to a common consumer group, regions in a topic are distributed among the respective instances. Upon the reception of a region, a service starts its specific processing instructions on the data partition of the respective region by loading the data from the key-value store using the region as the key.

\begin{figure}[b!]
    \centering
    \includestandalone{2_mainmatter/5_experimental_evalution/tikz/implementation_details}
    \caption[Implementation Details]{Operation Principles of a Processing Service}  \label{fig:implementation}
 \end{figure}

As the processing services effectively manage the disseminatation of data between all members of the CIDS, the data consistency between infrastructures depends on the processing guarantees given by the employed message queue that distributes the jobs among the processing instances. Kafka provides a configuration that enables transactional communication, such that a strong consistency can be achieved. For example, upon the successfull execution of operations on a data partition, the messaging queue can be notified and the job is done. In contrast to that, upon a failure, the messaging service will try to distribute that job again. Typically, the decision for transactional guarantees comes with the cost of a higher delay. Furthermore, for data availability and consistency on a datastore-level, the consistency guarantees of the employed data store is relevant. Redis implements a master-replica scheme with asynchronous replication and this provides an eventual consistency.

    % implementation code is provided as a repository in



\subsection{Methodologies}\label{subsec:methodologies}

% Describe Evaluation of the CIDS: Question of how does the CIDS improve attack detection in cases of local knowledge gaps? Does the exchange of generative models help to fill the knowledge gaps
     % leave one class out evaluation: simulate knowledge gaps by iteratively removing one class from the training proportion of one dataset while the testing proportion keeps all classes/samples; evaluate the fitted model by training on unvollständigen trainingsdatensatz, testen auf allen klassen (szenario ohne kollaboration); dann modell zusätzlich auf synthetischem Datensatz trainieren, der durch das CIDS bereitgestellt wird (szenario mit kollaboration);

     % verwendeter klassifizierer, train-test-split, klassifizierungsmetriken; binary + multiclass

     % establish a comparison baseline: two experiments shall show the baseline: first, a normal classification without removing classes from the train set is the ideal szenario
     % second, real data samples from other datasets instead of synthetic data should fill the knowledge gaps; shows what an exchange of flow samples could achieve and is the competitor for the presented appraoch in the contect of detection performance


% How well does the classification using label lookups in the case of simple regions (pattern lookups) work?
     % this experiments extend the kollaboration scenario with a specialized classifier that is tightly coupled with the Pattern Database; as simple regions are not represented by generative models, the classification information contained in these regions is not contained in the synthetic dataset; however, as described in Section X, the label information can simply be used for a constant lookup (vorrausgesetzt die eingehenden daten werden vorher gehasht!); das heißt, eingehende daten werden gehasht; falls dieser hash in der PDB enthalten ist und eine simple region darstellt, wird das entsprechende label als klassifizierungsergebnis verwendet; andernfalls wird der potente klassifierer (e.g. RandomForest) verwendet, um den flow zu klassifizieren


% Describe how to evaluate the compression capabilities
     % numper of parameters known per region (how many numbers to store)
     % thus, counting number of complex regions in the case of three datasets
     % compare this number to the number of samples*dim

\end{document}