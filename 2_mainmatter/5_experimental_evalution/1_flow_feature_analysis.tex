\documentclass[../../main.tex]{subfiles}
\begin{document}

\section{Network Flow Data}
First the employed datasets and the included attacks are presented shortly at the beginning in Section~\ref{subsec:employed_datasets}. After that, the feature extraction process is described in Section~\ref{subsec:feature_extraction}. There, it is described which flow exporter is used for the feature extraction and which features are obtained during the extraction. Section~\ref{subsec:preprocessing} present the preprocessing steps for the datasets, which include the labeling and data selection. For transparency, the distribution of different classes within each dataset is elaborated. A combined analysis of the feature importances for all datasets shows that no further feature selection would be beneficial, which finalizes the data description.

\subsection{Employed Datasets}\label{subsec:employed_datasets}

From the evaluation perspective, the employed datasets should both reflect similarities and diversity regarding the contained traffic in order to enable a realistic evaluation of the features of the PDB. Therefore, multiple IT-infrastructures that are members of the \gls{cids} are simulated by incorporating different benchmark datasets. The three following datasets are selected, which are from now on referred to by the corresponding roman numeral:

\begin{enumerate}
    \item[\RomanNumeralCaps{1}] CSE-CIC-IDS2018 \cite{cse-cic-ids-2018},
    \item[\RomanNumeralCaps{2}] CIC-IDS2017 \cite{sharafaldin_toward_2018},
    \item[\RomanNumeralCaps{3}] CIC-DoS2017 \cite{jazi2017detecting}.
\end{enumerate}

The most important criterion for the selection was, as already described at the beginning, the assurance of a non-exclusive existence of common classes among the datasets. Both dataset \RomanNumeralCaps{1} and \RomanNumeralCaps{2} are very alike from the included scenarios and attacks as they are designed as a diverse and comprehensive benchmark dataset originating from a common source. Nonetheless, they differ greatly with respect to the utilized testbed. Dataset \RomanNumeralCaps{2} consists of an attack network with four workstations and a victim network with three servers and ten workstations. In contrast to that, for the creation of dataset \RomanNumeralCaps{1} an attack network consisting of 50 workstations and a victim network consisting of 420 workstations and 30 servers were employed. Dataset \RomanNumeralCaps{3} is an intrusion detection dataset that contains besides the benign traffic only traces of application layer \gls{dos} attacks. This dataset differentiates from the others from the perspective of diversity with regard to the tools that were employed for the execution of the \gls{dos} attacks. However, a variety with regard to different attacks only exists for Dataset I and II. These attacks and, if relevant, how they are carried out are summarized below. 

The botnet scenario describes compromising systems so that they can be controlled as so-called bots by \gls{cnc} software. Subsequently, there are a number of different ways to abuse the system, such as installing backdoors, collecting and sending sensitive data, or using the computing resources to carry out a \gls{dos} attack. Two different trojan horse malware packages, namely Zeus\footnote{\url{https://github.com/ruCyberPoison/Zeus-Zbot_Botnet}} and Ares\footnote{\url{https://github.com/sweetsoftware/Ares}}, are used for creating the botnet traffic. Zeus was employed for the traces of dataset \RomanNumeralCaps{1}, whereas Ares was used for both dataset \RomanNumeralCaps{1} and \RomanNumeralCaps{2}.

The bruteforce scenarios include attacking logins via \gls{ftp}, \gls{ssh} and \gls{http} by using the Patator\footnote{\url{https://github.com/lanjelot/patator}} tool. In particular, a dictionary attack is executed. It is not known whether the login usernames are already known in advance. In all three datasets, \gls{dos} and \gls{ddos} scenarios were executed. The executed \gls{dos} attacks can be categorized into so-called high-volume and low-volume \cite{cambiaso2013slow} attacks based on \gls{http}. A \gls{ddos} attack has the same objective as a \gls{dos} attack, but its effectiveness is increased by using multiple compromised computer systems as the source of the attack. The \gls{ddos} traces of dataset \RomanNumeralCaps{1} include attacks generated by \gls{hoic} and \gls{loic}, whereas in the case for dataset \RomanNumeralCaps{2} only \gls{loic} was employed. \gls{hoic} targets the application layer by sending \gls{http} requests. \gls{loic} is capable of generating \gls{tcp} and \gls{udp} as well as \gls{http} traffic. By deploying the \gls{dvwa} on victim systems, web attacks such as SQL-Injection and \gls{xss} were executed against that application. The \gls{dvwa} is a web application that is designed to be attacked for, e.g. educational purposes. Furthermore, the Heartbleed\footnote{\url{https://heartbleed.com/}} exploit was executed on a server system. First, a vulnerable version of OpenSSL was compiled on the server. Subsequently, the server's memory was retrieved with the help of the Heartleech tool. Heartbleed is an exploit in the OpenSSL cryptography library. By sending a malformed hearbeat request to the respective receiver, the sender can read protected memory areas of the victim system. This allows, for example, to read encrypted communication or steal secret keys. Both dataset \RomanNumeralCaps{1} and \RomanNumeralCaps{2} contain portscan traffic generated by using a port scanner. A port scanner is software that can be used to check which services a system offers and on which ports they can be accessed. Furthermore, datasets \RomanNumeralCaps{1} and \RomanNumeralCaps{2} each contain so-called infiltration scenarios. These scenarios reflect complex attack strategies aimed at infiltrating the victim network from the inside. In both examples, a malicious \gls{pdf} file is sent to the victim system. Upon the usage of the file within a document reader, a vulnerability of the reader application is exploited. After the successful exploitation, a backdoor is executed on the victim system. Finally, the victim system is used for reconnaissance activities within the victim network.

The variety of different attacks present in the datasets are shown in Table~\ref{tab:preprocessed_flows} for reference. It can be seen that there is quite an extensive variety in the data. Furthermore, there are both large similarities and overlaps between the datasets in terms of attack types as well as large differences in terms of the testbed employed. Overall, the variance in the data is considered to be relatively large, providing a realistic representation of the spectrum of traffic that is expected from participants within the \gls{cids}.

\subsection{Feature Extraction}\label{subsec:feature_extraction}

In the context of network intrusion detection, network flow data is employed for real time network analysis. Within the context of real time flow feature analysis it is often neglected that flows are exported upon termination of the corresponding communication, which is either triggered by an activity or inactivtiy timeout mechanism. The specific setting of the timeout values is often a tradeoff between low data volume with a high timeout setting on the one side and a fine-grained analysis with low timeout values on the other side. However, especially in the area of network intrusion detection, a low detection latency heavily depends on timely available data. Thus, for a realistic evaluation, only early statistical flow features on the first 10 packets of a communication are considered. 

\begin{table}[t]
    \centering
    \footnotesize
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.1em}
    \input{table/number_exported_flows.tex}
    \caption[Exported Flows]{The number of the exported flows with the name of the respective capture files and the corresponding dataset.}
    \label{tab:num_exported_flows}
\end{table}

A flow exporter that is implemented using the Python Framework NFStream is used for tracking and exporting bidirectional network flows. A total of 45 flow features are extracted, including the basic 5-tuple and various statistical values based on the number of packets, packet sizes, and inter-arrival times. An active and inactive timeout of 15s is specified for the flow export. All datasets presented in Section \ref{sec:datasets} are available in the pcap capture file format. Files from the datasets CIC-IDS-2017 and CIC-DoS-2017 were exported directly. In the case of the CIC-IDS-2018 dataset further preparations were required. Some files contained broken headers, which were fixed with the pcapfix repair tool. In addition, the individual days, by which the dataset is structured, were fragmented further into many individual files, which were merged into single files using the pcapmerge tool. Subsequently, these files were exported. The number of the resulting flow samples are shown in Table \ref{tab:num_exported_flows}.

% although the datasets have common attack classes, they are strongly imbalanced regarding the number of samples per class as seen in Table \ref{tab:num_exported_flows} This mainly results from the differences in the architectures of the employed testbeds, where the traces were recorded in.



\begin{table}[t]
    \footnotesize
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.1em}
    \input{table/flow_features.tex} 
    \caption[Extracted Flow Features]{A number of 45 features are extracted from the network traffic, including the standard 5-tuple, timestamp and statistical early flow features}
    \label{tab:flow_features}
\end{table}


\subsection{Preprocessing}\label{subsec:preprocessing}
% labeling
The exported data flow samples are labeled based on basic attack knowledge (timestamps, IP addresses) from the respective website from which the datasets can be accessed. Improvements of the labeling process regarding corrected timestamps and further sanity checks, which are described in \cite{engelen2021}, have been taken into account. 
For example, % DoS Hulk is not incorporated as there are hints that the employed tool for producing theses traces does not work correctly;

% Data Selection/ Regrouping
Table \ref{tab:labeled_flows} shows the number of samples per class of each respective dataset. For the chosen evaluation strategy, the dataset is further preprocessed by grouping and renaming the samples that represent denial of service attacks. More precisely, already existing classes, that are labeled after the name of the specific tool that was used to create that attack, are relabeled by a strategy that is depicted in Table \ref{tab:class_groupings}. Furthermore, specific attacks that are either only represented in a single dataset (e.g. FTP brute force) or are too specific to be compared to (e.g. various infiltration attacks) are removed. The resulting classes and corresponding sample counts are shown in Figure \ref{tab:number_preprocessed_flows}. 

% as already mentioned class imbalance across the datasets is not furthert addressed in the preprocessing steps as this subject is handled in the Classifier Fitting Service of the Pattern Database 


\begin{table}
    \footnotesize
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.1em}
    \input{table/number_preprocessed_flows.tex} 
    \caption[Preprocessed Flows]{After the labeling process, the exported flow samples have partitioned to the classes presented.}
    \label{tab:preprocessed_flows}
\end{table}


% Feature Selection
In the next step, the data is analyzed regarding the feature importances; From these results, it can be derived, if a common feature subset across all datasets can be selected and by that improve the dataset;
Common advantages of feature selection are...
Recall, that both the feature set and the labels must be same across the different datasets in order for the CIDS to work.

permutation importance \cite[125]{rforests_2014}
mean decrease accuracy (MDA) or, equivalently, the mean increase error, of the forest when the values of a column of the dataset, i.e. feature, are randomly permutet in the out-of-bag-samples
"Permutation Importance or Mean Decrease in Accuracy (MDA) is assessed for each feature by removing the association between that feature and the target. This is achieved by randomly permuting the values of the feature and measuring the resulting increase in error. The influence of the correlated features is also removed."
\begin{figure}[t!]
    \centering
    \includestandalone{2_mainmatter/5_experimental_evalution/tikz/feature_importances}
    \caption[Flow Feature Importance Analysis]{Flow Feature Importance Analysis Using Mean Decrease Accuracy (MDA)}
    \label{fig:feature_importances}
\end{figure}

As seen in the plot, when considering all employed datasets combined, there is no common tendency for a subset of significant features that could be selected. Instead, each dataset has its own features that are more or less important for the classification result. Moreover, as this exemplary setup with only three different datasets already delivers this result, the chances are high that this trend increases with higher numbers of datasets, i.e., member in the CIDS

Thus, the complete set of extracted features is used for the information exchange within the CIDS; Intuitively that makes sense, because the initial goal was to provide a general knowledge database, which each member of the CIDS can incorporate in its detection pipeline with their individual preprocessing steps for the final classification training

\begin{algorithm}
    \caption{Permutation Importance}
    \label{alg:permutation_importance}
    \begin{algorithmic}[1]
        \REQUIRE fitted model $M$
        \ENSURE Feature importances $I$

        \STATE Compute the reference score $s$ of the model $M$ on data $\mathcal{X}$
        \FORALL{feature column $x_j$}
            \FORALL{repitition $i$ in $1, \dots K$}
                \STATE randomly shuffle column $j$ of dataset $\mathcal{X}$ to generate a corrupted version of the data named $\mathcal{\hat{X}_{i,j}}$
                \STATE compute the score $s_{i,j}$ of model $M$ on $\mathcal{\hat{X}_{i,j}}$
            \ENDFOR
            \STATE compute importance $i_j$ for feature $x_j$ $i_j = s - \frac{1}{K} \sum_i=1^K s_ij$
        \ENDFOR
        \STATE \textbf{return} $\mathcal{R}_{\text{out}}$
    \end{algorithmic}
 \end{algorithm}


\end{document}