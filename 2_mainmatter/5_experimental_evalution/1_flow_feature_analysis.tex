\documentclass[../../main.tex]{subfiles}
\begin{document}

\section{Network Flow Data}
First the employed datasets and the included attacks are presented shortly at the beginning in Section~\ref{subsec:employed_datasets}. After that, the feature extraction process is described in Section~\ref{subsec:feature_extraction}. There, it is described which flow exporter is used for the feature extraction and which features are obtained during the extraction. Section~\ref{subsec:preprocessing} present the preprocessing steps for the datasets, which include the labeling and data selection. For transparency, the distribution of different classes within each dataset is elaborated. A combined analysis of the feature importances for all datasets shows that no further feature selection would be beneficial, which finalizes the data description.

\subsection{Employed Datasets}\label{subsec:employed_datasets}

From the evaluation perspective, the employed datasets should both reflect similarities and diversity regarding the contained traffic in order to enable a realistic evaluation of the features of the PDB. Therefore, multiple IT-infrastructures that are members of the \gls{cids} are simulated by incorporating different benchmark datasets. The three following datasets are selected, which are from now on referred to by the corresponding roman numeral:

\begin{enumerate}
    \item[\RomanNumeralCaps{1}] CSE-CIC-IDS2018 \cite{cse-cic-ids-2018},
    \item[\RomanNumeralCaps{2}] CIC-IDS2017 \cite{sharafaldin_toward_2018},
    \item[\RomanNumeralCaps{3}] CIC-DoS2017 \cite{jazi2017detecting}.
\end{enumerate}

The most important criterion for the selection was, as already described at the beginning, the assurance of a non-exclusive existence of common classes among the datasets. Both dataset \RomanNumeralCaps{1} and \RomanNumeralCaps{2} are very alike from the included scenarios and attacks as they are designed as a diverse and comprehensive benchmark dataset originating from a common source. Nonetheless, they differ greatly with respect to the utilized testbed. Dataset \RomanNumeralCaps{2} consists of an attack network with four workstations and a victim network with three servers and ten workstations. In contrast to that, for the creation of dataset \RomanNumeralCaps{1} an attack network consisting of 50 workstations and a victim network consisting of 420 workstations and 30 servers were employed. Dataset \RomanNumeralCaps{3} is an intrusion detection dataset that contains besides the benign traffic only traces of application layer \gls{dos} attacks. This dataset differentiates from the others from the perspective of diversity with regard to the tools that were employed for the execution of the \gls{dos} attacks. However, a variety with regard to different attacks only exists for Dataset I and II. These attacks and, if relevant, how they are carried out are summarized below. 

The botnet scenario describes compromising systems so that they can be controlled as so-called bots by \gls{cnc} software. Subsequently, there are a number of different ways to abuse the system, such as installing backdoors, collecting and sending sensitive data, or using the computing resources to carry out a \gls{dos} attack. Two different trojan horse malware packages, namely Zeus\footnote{\url{https://github.com/ruCyberPoison/Zeus-Zbot_Botnet}} and Ares\footnote{\url{https://github.com/sweetsoftware/Ares}}, are used for creating the botnet traffic. Zeus was employed for the traces of dataset \RomanNumeralCaps{1}, whereas Ares was used for both dataset \RomanNumeralCaps{1} and \RomanNumeralCaps{2}.

The bruteforce scenarios include attacking logins via \gls{ftp}, \gls{ssh} and \gls{http} by using the Patator\footnote{\url{https://github.com/lanjelot/patator}} tool. In particular, a dictionary attack is executed. It is not known whether the login usernames are already known in advance. In all three datasets, \gls{dos} and \gls{ddos} scenarios were executed. The executed \gls{dos} attacks can be categorized into so-called high-volume and low-volume \cite{cambiaso2013slow} attacks based on \gls{http}. A \gls{ddos} attack has the same objective as a \gls{dos} attack, but its effectiveness is increased by using multiple compromised computer systems as the source of the attack. The \gls{ddos} traces of dataset \RomanNumeralCaps{1} include attacks generated by \gls{hoic} and \gls{loic}, whereas in the case for dataset \RomanNumeralCaps{2} only \gls{loic} was employed. \gls{hoic} targets the application layer by sending \gls{http} requests. \gls{loic} is capable of generating \gls{tcp} and \gls{udp} as well as \gls{http} traffic. By deploying the \gls{dvwa} on victim systems, web attacks such as SQL-Injection and \gls{xss} were executed against that application. The \gls{dvwa} is a web application that is designed to be attacked for, e.g. educational purposes. Furthermore, the Heartbleed\footnote{\url{https://heartbleed.com/}} exploit was executed on a server system. First, a vulnerable version of OpenSSL was compiled on the server. Subsequently, the server's memory was retrieved with the help of the Heartleech tool. Heartbleed is an exploit in the OpenSSL cryptography library. By sending a malformed hearbeat request to the respective receiver, the sender can read protected memory areas of the victim system. This allows, for example, to read encrypted communication or steal secret keys. Both dataset \RomanNumeralCaps{1} and \RomanNumeralCaps{2} contain portscan traffic generated by using a port scanner. A port scanner is software that can be used to check which services a system offers and on which ports they can be accessed. Furthermore, datasets \RomanNumeralCaps{1} and \RomanNumeralCaps{2} each contain so-called infiltration scenarios. These scenarios reflect complex attack strategies aimed at infiltrating the victim network from the inside. In both examples, a malicious \gls{pdf} file is sent to the victim system. Upon the usage of the file within a document reader, a vulnerability of the reader application is exploited. After the successful exploitation, a backdoor is executed on the victim system. Finally, the victim system is used for reconnaissance activities within the victim network.

The variety of different attacks present in the datasets are shown in Table~\ref{tab:preprocessed_flows} for reference. It can be seen that there is quite an extensive variety in the data. Furthermore, there are both large similarities and overlaps between the datasets in terms of attack types as well as large differences in terms of the testbed employed. Overall, the variance in the data is considered to be relatively large, providing a realistic representation of the spectrum of traffic that is expected from participants within the \gls{cids}.

\subsection{Feature Extraction}\label{subsec:feature_extraction}

 Within the context of real time flow feature analysis it is often neglected that flows are exported upon termination of the corresponding communication, which is either triggered by an activity or inactivtiy timeout mechanism. The specific setting of the timeout values is often a tradeoff between low data volume with a high timeout setting on the one side and a fine-grained analysis with low timeout values on the other side. 
 
 However, a low detection latency heavily depends on timely available data. Thus, high timeout values lead to a considerable delay of the data that is relevant to the intrusion detection pipeline, which is not acceptable in most cases. However, reducing the timeout values increases the data volume that is to be analyzed which in turn requires the corresponding computational resources. A compromise is the utilization of sub-flows. Additionally to the timeout values, exporting a sub-flow is triggered as soon as a specified number of packets have been transferred within the communication. In this example, instead of the whole packet stream, only the first 10 packets for both directions are considered, resulting in early statistical flow features. This way, the flow data is delivered in a timely manner while reducing the data volume. The only drawback of this approach is the loss of information relevant for the intrusion detection. 

\begin{table}[t]
    \centering
    \footnotesize
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.1em}
    \input{table/number_exported_flows.tex}
    \caption[Exported Flows]{The number of the exported flows with the name of the respective capture files and the corresponding dataset.}
    \label{tab:num_exported_flows}
\end{table}

A flow exporter that is implemented using the Python Framework NFStream is used for tracking and exporting bidirectional network flows. A total of 45 flow features are extracted, including the basic 5-tuple and various statistical values based on the number of packets, packet sizes, and inter-arrival times. A detailed listing of all features is presented in Table~\ref{tab:flow_features}. An active and inactive timeout of 15 seconds is specified for the flow export. All datasets presented in Section~\ref{subsec:employed_datasets} are available in the pcap capture file format. Files from the datasets \RomanNumeralCaps{2} and \RomanNumeralCaps{3} were exported directly. In the case of dataset \RomanNumeralCaps{1} further preparations were required. Some files contained broken headers, which were fixed with the pcapfix repair tool. In addition, the individual days, by which the dataset is structured, were fragmented further into many individual files, which were merged into single files using the pcapmerge tool. Subsequently, these files were exported. The number of the resulting flow samples are shown in Table \ref{tab:num_exported_flows}.

\begin{table}[H]
    \footnotesize
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.1em}
    \input{table/flow_features.tex} 
    \caption[Extracted Flow Features]{Flow features are extracted from the network traffic, including the 5-tuple, timestamp and statistical early flow features}
    \label{tab:flow_features}
\end{table}

Although the datasets have common attack classes, they are strongly imbalanced regarding the number of samples per class. This mainly results from the differences in the architectures of the employed testbeds, where the traces were recorded in.

\subsection{Preprocessing}\label{subsec:preprocessing}
% grey fields in the feature table is excluded
The exported flow samples are labeled based on domain knowledge, such as timestamps  and IP addresses, that is available on the respective website from which the datasets can be accessed. Improvements of the labeling process regarding corrected timestamps and further sanity checks, which are described in \cite{engelen2021}, have been taken into account. 
For example, traces that were produced by the DoS Hulk tool are neglected, because the tool is not implemented well and does not produce any meaningful attack data. Table~\ref{tab:preprocessed_flows} shows the number of samples per class of each respective dataset. Specific attack samples that are either not meaningful (DoS Hulk) or only represented in a single dataset (Brute-Force-FTP) or are too specific to be evaluated on (Infiltration) are removed, which is indicated by a grey cell color in the table.

For the chosen evaluation strategy, the dataset is further preprocessed by restructuring related attack samples and grouping them into a common class. For example, all samples that represent a low volume \gls{dos} attack that were produced by one of the many tools are grouped into a single class called ``\gls{dos} \gls{http} Low Volume''. Subsequently, the different classes receive a numeric label. The regrouping and relabeling of the attack classes across the different datasets enables a basis on which a reasonable communication within the \gls{cids} can happen in the first place. In practice, this approach would correspond to a centrally controlled policy that tells the individual members how their local datasets must be labeled before inserting them into the pattern database. The resulting classes and corresponding sample counts are shown in Figure \ref{tab:preprocessed_flows}. The already mentioned class imbalances across the datasets are not further addressed in the preprocessing steps as this subject is handled in the Classifier Fitting Service (see Section~\ref{subsec:classifier_fitting}).

In the next step, the data is analyzed regarding the feature importances. From these results, it can be derived, if a common feature subset across all datasets can be selected. Recall, that both the feature set and the labels must match across the different datasets in order for the \gls{cids} to work. Feature selection can be seen as a form of dimensionality reduction. Common advantages are an improved learning performance, an increased computation efficiency, a decrease in memory and storage consumption as well as better generalization capabilites of the resulting models \cite{li2017feature}. As a method for computing the feature importance the permutation importance, also called \gls{mda}, is selected. The \gls{mda} is assessed for each single feature by removing the association between the feature and the target by randomly permuting the values of the feature and measuring the increase in error that results from that \cite[125]{rforests_2014}. Algorithm~\ref{alg:permutation_importance} describes this procedure in detail.

Figure~\ref{fig:feature_importances} shows the permutation importances for each dataset, with the results being scaled to $[0,1]$ in order to establish comparability of the values across the datasets. It can be seen that there is no common tendency for a subset of significant features that could be selected. Moreover, as this exemplary setup with only three different datasets already delivers this result, the chances are high that this trend increases with higher numbers of datasets, i.e., members in the \gls{cids}. Thus, the complete set of extracted features is used for the information exchange within the \gls{cids}. Intuitively that makes sense, because the initial goal was to provide a general knowledge database, which each member of the \gls{cids} can incorporate in its detection pipeline with their individual preprocessing steps.

\begin{algorithm}[H]
    \caption{Permutation Importance}
    \label{alg:permutation_importance}
    \begin{algorithmic}[1]
        \REQUIRE Fitted model $M$
        \ENSURE vector $\bm{f}$ with feature importances as elements

        \STATE Compute the reference score $s$ of the model $M$ on data $\mathcal{X}$
        \FORALL{feature column $\{x_{d,1}, \dots, x_{d,N} \,|\, 1 \leq d \leq D\}$}
            \FORALL{iteration $j$ in $1, \dots J$}
                \STATE randomly shuffle column $d$ of dataset $\mathcal{X}$ resulting in corrupted data $\hat{\mathcal{X}}_{j,d}$
                \STATE compute the score $s_{j,d}$ of model $M$ on $\hat{\mathcal{X}}_{j,d}$
            \ENDFOR
            \STATE compute importance $\iota_d = s - \frac{1}{K} \sum\limits^K_{i=1} s_{j,d}$ and insert $\iota_d$ into $\bm{f}_d$
        \ENDFOR
        \STATE \textbf{return} feature importance vector $\bm{f}$
    \end{algorithmic}
 \end{algorithm}

 \begin{table}[H]
    \footnotesize
    \centering
    \setlength{\extrarowheight}{0pt}
    \addtolength{\extrarowheight}{\aboverulesep}
    \addtolength{\extrarowheight}{\belowrulesep}
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.1em}
    \input{table/number_preprocessed_flows.tex} 
    \caption[Preprocessed Flows]{After the labeling process, the exported flow samples have partitioned to the classes presented.}
    \label{tab:preprocessed_flows}
\end{table}

\begin{figure}[H]
    \centering
    \includestandalone{2_mainmatter/5_experimental_evalution/tikz/feature_importances}
    \caption[Flow Feature Importance Analysis]{Flow Feature Importance Analysis Using Mean Decrease Accuracy (MDA)}
    \label{fig:feature_importances}
\end{figure}

\end{document}