\documentclass[../../main.tex]{subfiles}
\begin{document}

\section{Discussion}\label{sec:discussion}

Looking first at the results of the baseline experiments, we see from Table~\ref{tab:missing_classes} that those classes that were removed from the training dataset cannot be detected by the classifier. This result is not very surprising, but it illustrates again the problem of isolated \gls{ids}. The second baseline experiment (see Table~\ref{tab:baseline_detection} is an alternative \gls{cids} in which attack data is exchanged directly among the participants. In addition to the data privacy problem, there is also the problem of high data volumes and the associated communication overhead. However, this experiment should first show whether it is at all possible to successfully replace missing data in local training datasets with attack data from other datasets. The results show a differentiated picture and a broad spectrum of compatibility degrees of the data among each other. For example, DDoS and bot traffic traces from different datasets are not compatible with each other at all, so that replacing them does not bring any advantage. However, it is also clear from the accuracy values that the exchange of data has at least not led to poorer detection of the other classes. On the contrary, there are certain classes for which the exchange in the \gls{cids} has shown a large positive effect. For example, the exchange of SSH brute force traffic samples resulted in the increase of the recall for this class from 0.0 to 0.9955 and 0.9813, respectively. Additionally, almost no other classes were falsely detected as SSH brute force traffic. This means that, at least in this scenario, no noise was created by the exchange of attack traffic. Across all classes where there was a positive effect, the average increase for the precision was 0.98 and for the recall 0.45. The exchange of brute-force web traces had a positive effect in the case of infrastructure I, but not in the opposite case in infrastructure II. A potential source for this circumstance could be differences in the test data set in each case. To confirm this assumption, further experiments would be necessary, e.g. with different splittings of the training and test data sets. Table~\ref{tab:num_samples_parameters} shows two things. 

First, it becomes clear that all three datasets differ significantly in the number of samples contained. On the other hand it becomes visible how significant the redundancy is in each case. Thus, the effect of deduplication is greatest for dataset III, where the number of samples is reduced by a factor of $5.6$. Table~\ref{tab:synthetic_classification} shows the results of the proposed CIDS architecture using synthetic data and without the use of pattern detection. The results not only demonstrate that the generative pattern dissemination can fill the knowledge gaps in local infrastructures, but that it provides better results than the direct exchange, although this is only valid for a hash size of 16-bit. The superiority of detection using synthetic data could be explained by improving the generalization of the model with respect to the problem. Since exactly as many samples were synthesized in the experiment as were indexed into the regions accordingly, an advantageous balancing effect over the baseline experiment is excluded. In general, the trend is that as the hash size increases, the advantages decrease. The reason for this could primarily be the amount of regions resulting from the different hash sizes. As the hash size increases, the number of regions also increases (see Table~\ref{tab:region_analysis}). Accordingly, the regions also contain fewer samples, since the fixed number of samples must be distributed among a larger number of regions. Consequently, the average amount of data available in a region is smaller and thus provides less input for the training of the generative model. The smaller amount of data could on average lead to a lower quality of the models. However, looking at the results of the classification with both synthetic data and the pattern detector in Table~\ref{tab:pattern_synthetic_classification}, it can be seen that an additional distinction between simple and complex regions during the classification hardly brings any advantages. On the contrary, the use of the pattern detector leads to a decrease in the detection performance compared to the results in Table~\ref{tab:synthetic_classification}. From the decreased accuracy it can be easily seen that the detection of all classes has deteriorated. In summary, this approach is on average even worse than the baseline scenario without any collaboration. The reader may have noticed that the locality sensitive hash function, which is used to create the regions, only uses different parameters for the OR-construction with respect to amplification. In other words, considering Algorithm~\ref{alg:lsh_preprocess}, the set of different hash functions $\{g_1, \dots, g_L\}$ in the presented approach corresponds to $L=1$. Increasing $L$ would most likely increase the recall for the corresponding classes, but would require an adaptation of the architecture and increase the memory consumption of the $PDB$. Table~\ref{tab:region_analysis} shows the extent to which the data volume of the information to be exchanged can be reduced by using the $PDB$. A hash size of 16 bits achieves the best values on average. However, it is not evident that there is a general trend for the data volume to increase as the hash size increases. This is only the case for dataset III. For dataset II, the exact opposite is the case. And the volume of dataset I first increases with increasing hash size and then decreases again. The number of parameters to be exchanged does not follow any visible pattern. Note, that complex regions contain among other things \glspl{gmm}, which can have in each case a different number of components, depending on the model selection. The number of components is the factor, which has the most effect on the number of the parameters of the model, since per component a mean vector and a covariance matrix must be stored. To reduce the number of parameters, a possible step would be to further reduce the maximum number of components within the model selection in the Generative Fitting Service. Nevertheless, it can be summarized that the use of the architecture with a hash size of 16-bit not only offers the best results in terms of attack detection, but also the best compression values. A reduction in data volume of up to less than 1\% of the original volume can be achieved. 


\end{document}