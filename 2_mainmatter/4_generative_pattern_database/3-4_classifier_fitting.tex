\documentclass[../../main.tex]{subfiles}
\begin{document}

\subsection{Classifier Fitting} \label{subsec:classifier_fitting}

The objective of the classifier fitting service is to provide the local infrastructures with the latest synthetic dataset for enhancing a local model fitting. Two different functional modes can be distinguished. First, as already described, a synthetic dataset is provided and the model that results from the respective fitting is used as the only detection algorithm. In addition to that, it is also possible to perform classifications directly with locality senstitive hashes from the pattern database. Thus, alongside the classifier model from the first scenario, a second detection algorithm  based on \gls{lsh} may be utilized in parallel. Therefore, in this section we first describe how to create a synthetic dataset using generative models. Subsequently, the classification with the help of the hashes is explained and formalized.

The goal is to ensure that changes to the $PDB_G$ are disseminated to the local infrastructures as quickly as possible. Thus, after each change to the generative models in the $PDB_G$ made by the previous service, an event is sent into all local communication channels. The classifier fitting service, that is executed locally within each infrastructure, receives these events within a defined time window and collects them. Upon the termination of the time window the classifier fitting service is initiated and begins with assembling a training dataset, if at least a single event was collected. All generative models are loaded sequentially from the $PDB_G$ and synthetic data is generated in the amount that was also originally found in the corresponding region by sampling from the respective \glspl{gmm}. After that, the synthetic data is transformed from the lower-dimensional subspace to the original data space by the inverse function of the original PCA operation using the stored parameters. In order to fit a classifier, the local training dataset is loaded from the respective $PDB_L$ and blended with the synthetic data. Subsequently, the inverse normalization operation is also applied on both the original and the synthetic data by loading the corresponding parameters that were used to normalize the data before indexing it. 


% For this task, all original data are first loaded from the local PDB. Then, 
% \begin{algorithm}
    % \caption{Dataset Assembly and Classifier Fitting}
    % \label{alg:classifier_fitting}
    % \algsetup{indent=2em}
 
    % \begin{algorithmic}[1]
    %     \REQUIRE Region Update Event $r$
    %     \ENSURE Model Update Notification
%    \end{algorithmic}
%  \end{algorithm}



% load the local dataset

% iterate through all regions and retrieve the respective model parameters

%       rebuild GMM and PCA models by deserialization and putting in parameters
%       sample as many samples per (GMM,PCA) as originally indexed into the respective region-label-infrastructure combination
%       collect (sample,label)-pairs in a collection

% combine local dataset and synthetic dataset
% shuffle, split
% fit Ensemble of DecisionTrees (RandomForest) on combined dataset
% serialize DecisionTree model and put into PDB
% put an event into local communication channel
% local IDS receives Event and retrieves lates DT-Model and puts it into the decision pipeline
\end{document}