\documentclass[../../main.tex]{subfiles}
\begin{document}

\subsection{Generative Fitting} \label{sec:generative_fitting}

The generative fitting service is the most demanding procedure in the context of processing resources. The service represents the filtering and compression operations presented in Section \ref{sec:high_level_overview}. There are two scenarios based on the region's complexity. If the region is not complex, no further actions are taken except it formally was complex. Then, existing models have to be deleted, since they are not longer used. And if the region is complex, generative models are provided, which represent the medium for exchanging information on attacks. More specifically, multiple \gls{gmm} are fitted on each label-subset of a region's data, which was stored in the indexing step in Section~\ref{sec:local_indexing}. According to a model selection process that evaluates the efficacy of each GMM, the best model is stored in the global pattern database, accessible to every member in the CIDS to sample synthetic data from. 

This way, every member has access to the global knowledge from all local infrastructures in order to enhance the local dataset that is used for fitting a classifier. Thus, this service is the key for providing \textit{privacy} and \textit{minimal overhead} while exchanging information. Considering the length and complexity of the complete algorithm, the main procedure is outlined first in Algorithm~\ref{alg:generative_fitting} and then the details on important sub-routines are elaboreted subsequently. 

Regions that have been updated in the preceding service are received as events $r \in \mathcal{R}_\text{in}$. Since the data is further organized per label within a region, the label set for a region $\mathcal{S}_r$ is retrieved. Additionally, the complexity state of the region $c_r$ is checked. Then, if the region is not complex, no model fitting is executed. Instead, potentially existing models are deleted from storage. This is the case, if the complexity status of the region has been changed from complex to simple. But if the region is currently complex, the generative model fitting procedure is triggered where new models are created and already existing models are updated. For that, from every hash table within a region, the original flow samples are extracted and collected in $\mathcal{X}_\alpha$, which is then stored in $T$ with the corresponding label as the key. The goal is to fit a model on each $\mathcal{X}_\alpha$ within a region, such that the collection of models for a region can be used in combination for sampling the synthetic data. Note that this is a specific process for the employed generativ algorithm. Since \acrshortpl{gmm} cannot be conditioned on multiple labels.

\begin{algorithm}
    \caption{Generative Fitting (Main Procedure)}
    \label{alg:generative_fitting}
    \algsetup{indent=2em}
 
    \begin{algorithmic}[1]
        \REQUIRE Regions $\mathcal{R}_{\text{in}}$
        \ENSURE Regions $\mathcal{R}_{\text{out}}$

        \STATE $\mathcal{R}_{\text{out}} \leftarrow \emptyset$
        \STATE $m \leftarrow$ getID()
        \FORALL{$r$ in $\mathcal{R}_{\text{in}}$}

            \STATE $\kappa \leftarrow \text{concatenate}(p_c, r)$
            \STATE $\delta \leftarrow \text{concatenate}(p_y, r, m)$

            \STATE $c_r \leftarrow PDB_G[\kappa]$
            \STATE $\mathcal{S}_r \leftarrow PDB_G[\delta]$ 

            \IF{$c_r = 0$}
                \FORALL{$y$ in $\mathcal{S}_r$}
                    \STATE $\omega \leftarrow \text{concatenate}(p_d, r, y, m)$ 
                    \STATE delete model in $PDB_G[\omega]$
                \ENDFOR
            \ELSE
                \STATE $T \leftarrow$ new hash table
                \FORALL{$y$ in $\mathcal{S}_r$}
                    \STATE $\alpha \leftarrow \text{concatenate}(p_x, r, y)$
                    \STATE $\mathcal{X}_\alpha \leftarrow \text{ extract values (flows) from } PDB_{L_m}[\alpha]$
                    \STATE $T[y] \leftarrow \mathcal{X}_\alpha$
                \ENDFOR

                \STATE $T \leftarrow \text{upsampling}(T)$

                \FORALL{$(y, F_y)$ in $T_F$}
                    \STATE $\text{GMM} \leftarrow \text{modelSelection}(y, F_y)$
                    \STATE $\omega \leftarrow \text{concatenate}(p_d, r, y, m)$ 
                    \STATE $PDB_G[\omega] \leftarrow \text{GMM}$
                \ENDFOR
                \STATE add $r$ to $\mathcal{R}_{\text{out}}$
            \ENDIF
        \ENDFOR
        \RETURN $\mathcal{R}_{\text{out}}$
    \end{algorithmic}
 \end{algorithm}

 
 Note that this is a specific process for the employed generative algorithm. Since \acrshortpl{gmm} cannot be conditioned on multiple labels, multiple single label models have to be fitted. Subsequently, upsampling operations prepare the collected region data in $T$ for the model fitting (see Algorithm \ref{alg:upsampling}). After that, the model selection process is started sequentially for each available label in the region (see Algorithm~\ref{alg:model_selection}). Finally, the best fitted model is stored in the $PDB_G$. 


 So far, the main procedure has been outlined. Next, the details on the upsampling are elaborated (see Algorithm \ref{alg:upsampling}). As each $\mathcal{X}_\alpha$ is used for the fitting of a \gls{gmm} and since some of these flow data subsets of a region potentially exhibit a low number of samples, an upsampling process may need to applied. The upsampling process ensures that the fitting algorithm for the GMM is able to work. For that, the number of samples has to be at least equal to the number of components of the \gls{gmm} which in turn is at most equal to the number of dimensions of the training dataset $\mathcal{X}_\alpha$. And since different parameters for the number of components are tried during the model selection, the upper limit for the parameter is used as a comparison value. In this case the upper limit for the number of components is the number of dimensions of the training data. If this is not the case, the upsampling fills the lacking samples artificially. 

 For the initialization of the responsibilities of the mixture components within the fitting procedure of the \gls{gmm} the k-means algorithm is used to find distinct clusters within $\mathcal{X}_\alpha$. The resulting cluster assignments for each $\bm{x} \in \mathcal{X}_\alpha$ are then used as initial values for the responsibilities. A supersampling process results in duplicate points that could lead to a smaller number if distinct clusters found by the k-means algorithm than the number of components specified in the model. Therefore, the upsampling is realized by generating near neighbours per sample.

 First, the number of neighbours to generate per sample $N_\Delta$ is determined by dividing the difference of the dimensionality of the flow data $\text{dim}(\mathcal{X}_\alpha)$ and the number of data points $|\mathcal{X}_\alpha|$ by $|\mathcal{X}_\alpha|$ using an integer division. This ensures that each original data point $\bm{x}$ contributes an equal share to the total amount of newly sampled points $\hat{\bm{x}}$. In case, the result of the integer division is zero, one is added to the result. Then for each data point $\bm{x} = [x_1, \dots, x_D]^\top$, nearest neighbours are generated by sampling from a uniform distribution $\mathcal{U}(a, b)$. 

 \begin{algorithm}
    \caption{Binary Splitting and Upsampling of Region Data}
    \label{alg:upsampling}
    \algsetup{indent=2em}
 
    \begin{algorithmic}[1]
        \REQUIRE Hash table $T$ with labels $y$ as keys and sets of flows $\mathcal{X}$ as values
        \ENSURE Hash table $T$ with upsampled sets of flows
        \FORALL{$(y, \mathcal{X}_\alpha)$ in $T$}
            \IF{$y$ $\neq 0$ and $|\mathcal{X}_\alpha| < \text{dim}(\mathcal{X}_\alpha)$}               
                \STATE $\hat{\mathcal{X}} \leftarrow \emptyset$
                \STATE $N_\Delta \leftarrow \lfloor \text{dim}(\mathcal{X}_\alpha) - |\mathcal{X}_\alpha| \, / \, |\mathcal{X}_\alpha| \rfloor + 1 $
                \FORALL{$\bm{x}$ in $\mathcal{X}_\alpha$}
                    \FORALL{$n$ in $\{ 1 \leq n \leq N_\Delta \}$}
                        \STATE $\hat{\bm{x}} \leftarrow [s(x_1), \dots, s(x_D)]^\top$ with $s(x_d) \sim \mathcal{U}(a, b)$
                        \STATE add $\hat{\bm{x}}$ to $\hat{\mathcal{X}}$
                    \ENDFOR
                \ENDFOR
                \STATE $\mathcal{X}_\alpha \leftarrow \mathcal{X}_\alpha \cup \hat{\mathcal{X}}$
                \STATE $T[y] \leftarrow T[y] \cup \mathcal{X}_\alpha$ 
            \ENDIF
        \ENDFOR
        \RETURN $T$
    \end{algorithmic}
 \end{algorithm}
 
 That way, the nearest neighbour of $\bm{x}$ is the concatenation of the sampled feature values as $\hat{\bm{x}} = [s(x_1), \dots, s(x_D)]^\top$, where $s(x_d) \sim \mathcal{U}(a, b)$ is defined as the value sampled from the uniform distribution within the interval $[x_d-\epsilon, x_d+\epsilon)$. The value $\epsilon$ controls the size of the interval of the uniform distribution. The larger the value for $\epsilon$, the further the newly generated values deviate from the orginal feature values. In Figure \ref{subfig:hist_nn} the value for a single feature $x_d$ is drawn uniformly at random. The original value of the feature was $x_d=4$, which was extended by $\epsilon=1\cdot10^{-2}$. By choosing a relatively small value for $\epsilon$, it is ensured that the generated nearest neighbours do not alter the original data distribution significantly, while enabling a more ideal fitting of the GMM for that set of data points. Finally, the generated neighbours and the original data points are combined. After processing each pair $(y, \mathcal{X}_\alpha)$, the hash table $T$ is returned to the main procedure (Algorithm~\ref{alg:generative_fitting}) for the subseqeuent model selection.

 \begin{figure}[t!]%
    \centering%
    \begin{subfigure}[b!]{0.49\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/4_generative_pattern_database/tikz/uniform_distribution}%
        \caption{The pdf of a uniform distribution is $p(x) = \frac{1}{b-a}$ within the interval $[a, b)$, and zero elsewhere.}%
        \label{subfig:uniform_dist}%
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b!]{0.49\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/4_generative_pattern_database/tikz/histogram_nearest_neighbour}%
        \caption{Histogram of \numprint{1000} samples drawn uniformly over the interval $[x_d-\epsilon, x_d+\epsilon)$ where $x_d=0.4$ and $\epsilon=1 \cdot 10^{-2}$.}%
        \label{subfig:hist_nn}%
    \end{subfigure}%
    \caption{Nearest neighbours are generated by sampling each feature value from a the uniform distribution.}%
    \label{fig:uniform_dist_nn}%
\end{figure}%


 % That way, the models are only fitted on data with the label in focus but evaluated with the complete region data.

 
 %Moreover, as stated in Section (section of GMM), the runtime of a single step of the EM algorithm in this setting is asymptotically $O(NKd^3)$ or $O(NKd^2)$ by using the incremental algorithm proposed in \cite{pinto2015fast} ($N$ data points, $K$ components and $d$ dimensions); therefore, this curse of dimensionality that is encountered in calculating the covariance matrix while fitting the GMM is coped by approximation; using the diagonal covariance matrix as approximation to the regular covariance matrix; 
 

 In the next phase of the algorithm, one \acrshort{gmm} is selected per unique label within a region. During the selection process multiple models are fitted with different model parameters using a flow data subset $\mathcal{X}_\alpha$. Since the resource demands of the \gls{ema} for fitting a \acrshort{gmm} are relatively high, the dimensionality of the training data is reduced by applying a dimensionality reduction via \gls{pca}. Later in the pipeline, when sampling data from the \acrshortpl{gmm}, the inverse operation of the corresponding \gls{pca} is applied on the synthetic data in order to restore the original feature space. Therefore, the parameters of the \gls{pca} have to be stored in the $PDB_G$ along with the parameters of the \gls{gmm}. 
 
 Note, that during the application of \gls{pca} in the first place, information within the data is lost by only keeping the components that describe most of the variance of the data. Of course, the result of the inverse transformation would then only be an approximation of the original data. However, the input for the inverse transformation is going to be the synthetic data. Thus, we accept the loss and focus on the fact that the synthetic data is transformed back into the original feature space, such that the synthetic data resembles more the original data as it was captured, increasing its compatibility.
 
 The model selection is specified in Algorithm~\ref{alg:model_selection}. First \gls{pca} is applied on $\mathcal{X}_\alpha$ resulting in a set of flows $\mathcal{X}'_\alpha$ with a reduced feature set. The number of components of the \gls{pca} has to be set heuristically or a threshold has to be set, which defines the minimum amount of variance that needs to be explained by a specific number of components. After the compression, the parameter search for the \gls{gmm} is started. In general, there is no exact method to determine the optimal parameters for a \gls{gmm}, such that different parameter settings have to be evaluated. For that, the search space is defined by two dimensions, namely the number of components and the type of covariance matrix that is used for each component. The set of numbers that is used for specifying the number of components is defined as $\mathcal{C} = \{ 4c \, | \,  c\in \mathbb{N}, 1 \leq c \leq \text{dim}(\mathcal{X}_1) \}$. The covariance matrix can either be a standard covariance matrix with a full set of entries or can be an approximation with entries only on the diagonal of the matrix, i.e., we define $\bm{\Sigma} = \{\text{``full''}, \text{``diagonal''}\}$. The reason behind the choice of both types of covariance matrix is also motivated by heuristical optimization. The full covariance matrix can result in overfitting, especially on small datasets, whereas the diagonal matrix acts as a type of regularization to the model. However, depending on the data the diagonal type sometimes also leads to an underfitting of the model.

 After fitting a \gls{gmm} on $\mathcal{X}'_1$ the first metric for the selection is calculated. Precisely, the \gls{bic} is calculated as in Equation~\ref{eq:bic} (see Section~\ref{subsec:em_algorithm}). Note, that for the case of a diagonal covariance matrix, the number of entries of the covariance matrix to estimate changes from $\frac{D(D+1)}{2}$ to $D$. Thus, given a \gls{gmm} with $K$ mixture components, each exhibiting a diagonal covariance matrix, and a datasset $\mathcal{X}_1$ consisting of $N$ data points and $D$ features, the \gls{bic} is defined as

 \begin{equation}
     \text{BIC}(\text{\gls{gmm}}|\mathcal{X}_1) = (KD+D+K-1) \, \text{ln}(N) - 2 \, \text{ln}(\hat{L}).
 \end{equation}

 After that, a second evaluation metric is calculated (see Algorithm~\ref{alg:adversary_evaluation}). The process that is used to gather that metric is called adversary evaluation and is based on the principles of \Acrfullpl{gan}.
 


\begin{algorithm}
    \caption{Model Selection}
    \label{alg:model_selection}
    \algsetup{indent=2em}
 
    \begin{algorithmic}[1]
        \REQUIRE Dataset $\mathcal{X}_1$
        \ENSURE Tuple $(\text{\gls{gms}, \gls{gmm}, \gls{pca}})$

        \STATE $\mathcal{X}'_1 \leftarrow \text{PCA}(\mathcal{X}_1)$
        \STATE $\mathcal{C} \leftarrow \{ 4c \, | \,  c\in \mathbb{N}, 1 \leq c \leq \text{dim}(\mathcal{X}_1) \}$
        
        \STATE $\bm{\Sigma} \leftarrow \{\text{``full''}, \text{``diagonal''}\}$

        \STATE $L \leftarrow$ new list

        \FORALL{$c$ in $\mathcal{C}$}
            \FORALL{$\Sigma$ in $\bm{\Sigma}$}
                \STATE \gls{gmm} $\leftarrow \text{fitGaussianMixture}(\mathcal{X}_1', c, \Sigma)$
                \STATE \gls{bic} $\leftarrow \text{\gls{bic}}(\text{\gls{gmm}}, \mathcal{X}_1')$
                \STATE $\text{\gls{acc}} \leftarrow$ AdversaryEvaluation$(\mathcal{X}_1, \text{\gls{gmm}}, \text{\gls{pca}})$
                \STATE $\text{\gls{gms}} \leftarrow \text{\gls{gms}}(\text{\gls{bic}}, \text{\gls{acc}})$
                \STATE add $(\text{\gls{gms}, \gls{gmm}, \gls{pca}})$ to $L$
            \ENDFOR
        \ENDFOR
        \STATE sort $L$ according to the values of \gls{gms} 
        \RETURN $(\text{\gls{gms}, \gls{gmm}, \gls{pca}})$ with the lowest value for \gls{gms}
    \end{algorithmic}
 \end{algorithm}

 \begin{figure}[t!]%
    \centering%
    \begin{subfigure}[b]{0.49\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/4_generative_pattern_database/tikz/adversary_evaluation_2}%
        \caption{A poor fitted generative model: The original data (triangles) and the synthetic data (circles) can be easily separated by a classifier.}%
        \label{subfig:adversary_evaluation_bad}%
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.49\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/4_generative_pattern_database/tikz/adversary_evaluation}%
        \caption{A well fitted generative model: The synthetic data resemble the original data well, such that a separation is not easy.}%
        \label{subfig:adversary_evaluation_good}%
    \end{subfigure}%
    \caption{Adversary evaluation: seperate original and fake data.}%
    \label{fig:adversary_evaluation}%
\end{figure}%

Fitting a \gls{gan} involves a discriminator function, i.e., classifier that tries to distinguish between the original and synthetic data. In other words, the generative algorithm synthesizes fake data, which resembles the original data. Based on that, a dataset is assembled by combining fake data labeled as the positive class $(1)$ and original data labeled as the negative class $(0)$ or vice versa. This dataset is shuffled and split into a training and test proportion.

Then, a discriminative model is trained on the training proportion, trying to find distinguishing features within both classes, allowing a distinction to be made. Within the evaluation step, the discriminator is tested on its ability to separate fake and original data. The lower the score of the discriminator, the better the generative model. Figure \ref{fig:adversary_evaluation} illustrates two scenarios. In the first scenario (\subref{subfig:adversary_evaluation_bad}), the generative model has been fitted poorly, because the synthetic data can be easily separated from the original data as indicated by the decision boundary (dashed line). In the second scenario (\subref{subfig:adversary_evaluation_bad}), the generative model performs well by creating synthetic data that could not be seperated from the original data. 

\begin{algorithm}
    \caption{Adversary Evaluation}
    \label{alg:adversary_evaluation}
    \algsetup{indent=2em}
 
    \begin{algorithmic}[1]
        \REQUIRE $\mathcal{X}_1, \text{GMM}, \text{PCA}$
        \ENSURE $ACC$ from decision tree model $DT$

        \STATE $\tilde{\mathcal{X}}'_1 \leftarrow$ sample $|\mathcal{X}_1|$ data points from \gls{gmm}

        \STATE $\tilde{\mathcal{X}}_1 \leftarrow \text{PCA}^{-1}(\tilde{\mathcal{X}}'_1)$ 

        \STATE $\mathcal{Y} \leftarrow \{ 1 \}$
        
        \STATE $\tilde{\mathcal{Y}} \leftarrow \{ 0 \}$

        \STATE $\mathcal{D} \leftarrow \text{concatenate}(\mathcal{X}_1 \times \mathcal{Y}, \tilde{\mathcal{X}}_1 \times \tilde{\mathcal{Y}})$
        
        \STATE $\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{test}} \leftarrow \text{shuffleSplit}(\mathcal{D})$

        \STATE DT $\leftarrow \text{fitDecisionTree}(\mathcal{D}_{\text{train}})$

        \STATE $\hat{\mathcal{Y}} \leftarrow$ predict $\mathcal{X}_{\text{test}} \subset \mathcal{D}_{\text{test}}$ with DT

        \STATE $ACC \leftarrow ACC(\mathcal{Y}_{\text{test}}, \hat{\mathcal{Y}})$

        \RETURN $ACC$
    \end{algorithmic}
 \end{algorithm}

In order to quantify the performance of the discriminator within the adversary evaluation, a classification metric is calculated. Since the original and synthetic data is equal in size and the posed problem is binary the \acrfull{acc} is chosen, which is defined as

 \begin{equation}
    \text{\gls{acc}} = \frac{TP + TN}{TP + TN + FP + FN}
 \end{equation}

 For each iteration in the model selection process, both the \gls{bic} and the \gls{acc} are stored. After all parameter combinations have been exhausted and all resulting models have been evaluated a combined metric that we call  \acrfull{gms} is calculated from the \gls{bic} and \gls{acc} for each model. For that, the value range of both metrics must be aligned, such that the \gls{bic} is normalized into the range $[0, 1]$ by applying the min-max-normalization from Equation \ref{eq:normalization}. After that, the mean is calculated for each metric pair as defined in Equation~\ref{eq:combined_metric}. Finally, the model with the lowest combined \gls{gms} is selected. 
 
 \begin{equation}\label{eq:combined_metric}
    \text{\gls{gms}} = \frac{1 - \text{\gls{acc}} + \text{\gls{bic}}}{2}.
 \end{equation}
 
 The model parameters of both the \gls{gmm} and the \gls{pca} are stored in the $PDB_G$. In numbers, these can range, depending on which type of covariance matrix is chosen for the \gls{gmm}. \Acrshort{pca} on the other hand, requires a $D \times M$ projection matrix, with $D$ being the dimensions of the original data space and $M$ being the number of dimensions in the projected subspace. Note that $|\mathcal{S}_y|$ defines the number of unique labels within a region, which effectively determines the number of model pairs $(\text{\gls{gmm}, \gls{pca}})$ that are to be stored per region. Therefore, per region, the number of parameters to store is at least
 
 \begin{equation*}
    |\mathcal{S}_y| \, ( 2KD +K-1 + MD  )
 \end{equation*}

 and at most

 \begin{equation*}
    |\mathcal{S}_y| \, \bigl( (KD + K \, \frac{D(D+1)}{2}+K-1) + MD \bigr).
 \end{equation*}

 % store the number of original samples 









\end{document}