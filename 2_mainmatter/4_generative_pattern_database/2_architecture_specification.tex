\documentclass[../../main.tex]{subfiles}
\begin{document}

\section{Architecture Specification} \label{sec:architecture_specification}

\begin{figure}[b!]
    \centering
    \includestandalone{2_mainmatter/4_generative_pattern_database/tikz/detailed_architecture}
    \caption[Proposed \gls{cids} architecture]{\gls{cids} architecture illustrating the data and event flow between components within and across local infrastructure boundaries.}
    \label{fig:detailed_architecture}
    \end{figure}

    Logically, the proposed CIDS exhibits a hierarchical architecture (see Figure \ref{fig:detailed_architecture}). For one, the global infrastructure $G$ represents the collection of $M$ CIDS participants and their knowledge on an abstract level. For another, it provides specific services, that are globally available to each local infrastructure $L_m, m \in \{1, \dots, M\}$ that includes all CIDS components and services within the IT infrastructure boundaries of a corresponding CIDS member. Each local infrastructure $L_m$ agrees to a specified feature extraction process that provides the monitoring data $\mathcal{X} \subset \mathbb{R}^D$ with a total number of features $D = dim(\bm{x})$ for the attack detection. As the deployed NIDS is considered to be a supervised classification task, the ultimate goal is to find an approximation of an unknown target function $c: \mathcal{X} \rightarrow \mathcal{Y}$ that maps samples $\bm{x} \in \mathcal{X}$ to classes $y \in \mathcal{Y}$. In order to find an approximation $\hat{c} \sim c$, an individual training dataset $\mathcal{D}_m= \{(\bm{x}_n, y_n): 1 \leq n \leq N_m\}$ of size $N_m = |\mathcal{D}_m|$ is provided in every $L_m$. True target values $c(\bm{x}) = y$ are given by the domain expert that assembled $D_m$. In addition, in the CIDS network, a common $\mathcal{Y}$ is agreed upon, so that there is a global consensus on class memberships.
    
    CIDS communication across local boundaries occurs exclusively in a vertical direction. Thus, the exchange of information between individual $L_m$ takes place indirectly via the global pattern database $(PDB_G)$ and the global event channel $(C_G)$. Each $L_m$ includes a local pattern database $(PDB_{L_m})$, a local event channel $(C_{L_m})$ and an event-based data processing pipeline that consists of four services, namely \textit{Local Indexing}, \textit{Complexity Estimation}, \textit{Generative Fitting} and \textit{Classifier Fitting}. 
    
    Each instance of a pattern database $(PDB)$ is realized as a key-value store and depending on the scope different tasks are considered. Each $PDB_{L_m}$ is responsible for storing $D_m$ and corresponding metadata. The $PDB_G$ stores global metadata and the generative models that resemble the original data from each $D_m$. The notation for operations on a $PDB$ corresponds to the notation for hash table operations defined in Section~\ref{subsec:locality-sensitive-hashes}. Note, that if a locality-sensitive hash function is used to construct a key, in practice a hash table will internally apply a non-cryptographic hash function on the key to ensure an even distribution of the keys across the slots.
    
    Nonetheless, the clustering of neighbouring points into common slots is still ensured in this scenario. Each event channel $C$ provides a topic-based publish-subscribe messaging mechanism that is mainly used to instantiate and distribute workloads among the service instances in the processing pipeline. Via the messaging system, service instances receive and emit events, on which upon the respective operations are triggered. Changes in a $PDB$ result in responses that in turn are leveraged as the respective events. In this fashion, updates are propagated throughout the processing pipeline, ensuring a timely consistency among the pattern databases.
\end{document}