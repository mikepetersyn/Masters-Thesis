\documentclass[../../main.tex]{subfiles}
\begin{document}

\chapter{Related Work}

This chapter presents related work organized in three categories. First, data disseminatation in \gls{cids} is reviewed in Section~\ref{sec:data_disseminatation_cids}. Two interesting approaches are examined in detail. Based on this, the main challenges in this domain are summarized. Second, Section~\ref{sec:similarity_hashing_intrusion_detection} addresses the topic of similarity hashing in intrusion detection. In this context, the similarities and differences between approximate matching and \gls{lsh} are first discussed. This is followed by a broad overview of various approaches that use similarity hashing in the context of intrusion detection. Third, section~\ref{sec:generative_algorithms_ids} investigate which use cases exist for generative algorithms in the field of intrusion detection. Finally, the main features of the presented approach are comparatively summarized based on the focus areas that appear in the related work in Section~\ref{sec:generative_algorithms_ids}.

\newpage
\section{Data Dissemination in CIDS}\label{sec:data_disseminatation_cids}

Among the key components of a \gls{cids} described in Section~\ref{subsec:cids}, data dissemination is particularly noteworthy as one of the fundamental mechanisms for the communication between members. A central aspect is the communication \textit{overhead} that is introduced with the dissemination of information within a CIDS, which in turn is heavily influenced by the \gls{cids} architecture \cite[p.39]{vasilomanolakis_collaborative_2016}. The data dissemination strategy of a \gls{cids}  not only defines the communication paths as described above, but also influences what kind of data is exchanged between the \gls{cids}  members while also specifying format and level of granularity. This also includes the central aspects of \textit{data privacy}, realized by utilization of, e.g., bloom filters \cite{Vasilomanolakis2015SkipMon}\cite{Locasto2005}, and \textit{interoperability} that can be obtained by standardized formats, such as the \gls{idmef} \cite{debar2007intrusion}. For example, the architecture in \cite{Cuppens2002} implements an alert base management function that receives alert messages from different IDS which are subsequently stored for further analysis. It is assumed that the messages are compliant with the IDMEF. Messages are converted into a set of logical facts and stored in a relational database upon reception, such that a correlation function can operate on the data. Another example is the P2P overlay \gls{ids}, presented in \cite{Duma2006}. Here, different isolated \gls{ids} cooperate within a P2P network in order to collectively detect attacks. Upon an attack, a node warns its peers about that attack with a warning message. To assure interoperability the \gls{idmef} is employed for sending warning messages. In the context of communication overhead, there are two approaches in particular to mention. In \cite{Locasto2005}, the authors present an approach for the efficient and privacy aware distribution of alert data in a distributed \gls{cids}. Before exchanging information, the respective data is compressed by the utilization of a bloom filter data structure. Upon an alert, the relevant information, e.g. IP address, is inserted ino the bloom filter. Since the bloom filter contains information on suspicious hosts, it is called \textit{watchlist}. The watchlist is shared among peers, which are selected by a network scheduling algorithm called \textit{whirlpool}, that dynamically creates an overlay that defines peer relationships. The authors mention two challenges in the context of data dissemination. First, sharing sensitive information is not an option, thus preventing participation in the CIDS. Second, the tradeoff between latency of information exchange and the reliability of the exchange must be balanced. Centralized approaches disseminate information reliable and predictable, but they constitute a bottleneck. While distributed approaches scale well, information can be lost or delayed by partitioning the data among the peers. In the context of reducing communication overhead, this approach addresses this challenge twofold. First, alert data is compressed when inserted into the bloom filter by hashing. Second, only peers exchange data, which reduces overhead significantly, when compared to a complete distribution. However, bloom filters are a probabilistic data structure, that exhibits an increasing false positive rate with an increasing filling degree. Thus, when scaling this approach, the probability that innocent hosts are considered as malicious, increases. Furthermore, the authors in \cite{Vasilomanolakis2015SkipMon} state, that a CIDS needs to provide \textit{scalability}, minimal message \textit{overhead}, \textit{privacy} of the exchanged alert data, an \textit{domain awareness}, which describes the ability to constrain alert disseminatation to specific sub-domains of a network. Instead of randomly creating sub-domains as in \cite{Locasto2005}, the authors suggest to incorporate network traffic similarities into account for this process. The communication within this approach is done within a P2P overlay network. 

First, specific features from alerts are extracted and added into a bloom filter. Then, a defined data dissemination technique (e.g. flooding, partial flooding, gossipping) is utilized for sending bloom filters to other nodes. When a node receives data, it computes a similarity value by performing logical operations on the respective bloom filters. By doing so, each sensor creates a matrix with its local knowledge of other sensors. Based on this knowledge and along with a similarity threshold, sensors can identify similar nodes and form a community with them to, afterwards, exchange more fine-grained alert data. One problem in this approach is finding an optimal value for the similarity threshold. Furthermore, the formation of communities could be generalized if the similarity threshold did not depend on specific alerts, but rather on the general network characteristics of an infrastructure. To sum up, four challenges are considered to be essential in the context of data dissemination in \gls{cids}. First, the computational overhead introduced by the communication of multiple monitors in the CIDS needs to be minimized, such that potential knowledge gains are available fast enough. Second, members may not want to disclose data that contains information on system- and network states of their infrastructure, as it constitutes a privacy and security problem. This includes, among other things, legal aspects when it comes to sharing log and network data. Nonetheless, the exchange of this information is crucial for the effective operation of a \gls{cids}. Furthermote, interoperability is constitutional as the individual members in the \gls{cids} should be able to interact with each other. Lastly, the aspect of domain awareness supports the minimization of overhead. By constraining the
flow of information, the communication volume is decreased, while at the same time
ensuring the dissemination to relevant members.

\section{Similarity Hashing in Intrusion Detection}\label{sec:similarity_hashing_intrusion_detection}

With the term similarity hashing we summarize all methods that are known by the name approximate matching or \gls{lsh}, although there are more or less subtle differences in these approaches. Nonetheless, this section presents works in the field of intrusion detection using methods that belong to the category of similarity hashing. First, the differences between the two methods are briefly described. Starting with \gls{lsh}, recall from Section~\ref{subsec:locality-sensitive-hashes} that this is an algorithm for solving the \gls{cnn} which exhibits strong theoretic assumptions. Furthermore, it solves the problem by clustering data points according to a given distance measure, which in fact defines the similarity. Approximate matching functions, on the other hand, are designed to identify similarities between two objects by creating digests of the respective objects and comparing them with each other. Approximate matching funtions are not only capable of finding objects that resemble each other but also find objects that are contained in another object \cite{breitinger2014approximate}. Note that existing definitions are blurry and hybrid constructions, such as TLSH, which is an algorithm that uses \gls{lsh} to build an approximate matching function \cite{oliver2013tlsh}, also exist. The field of cyber security has adopted methods for similarity search, mainly for the analysis of polymorphic malware. This type of malware poses a significant challenge, since it changes its appearance over time in order to stay undetectable from antivirus software \cite[p.91]{whitman_principles_2018}. Traditional antivirus software uses cryptographic hash functions in order to create file signatures. Such signatures are well suited to search for identical files in a knowledge database. However, due to the property of cryptographic diffusion of cryptograhpic hash functions, even minimal changes to the malware result in large differences in the resulting hash value. 

With the rapid evolution and proliferation of polymorphic malware, detection based on unique signatures no longer seems effective. Based on these developments, detection schemes using approximate matching functions have initially become the focus of research. Popular approaches in this area are for example \textit{ssdeep} or \textit{sdhash}. Ssdeep \cite{kornblum_identifying_2006} is an established algorithm for malware detection as it is supported by the malware analysis repository VirusTotal [insert url]. The algorithm evolved from the spamsum algorithm [?] that was designed for detecting spam. Ssdeep implements context triggered piecewise hashing (CTPH), which is a form of block-based hashing. Block-based hashing seperates the data into fixed-size blocks, that are hashed individually, commonly by a non-cryptograhpic hash function. The final digest results from the concatenation of all hashes. The similarity of two digests can then be measured by comparing the number of common blocks. Existing algorithms mainly differentiate by their approach for the block construction. In particular, ssdep uses a rolling hash to create variable-sized blocks. A sliding window moves through the input and produces specific outputs based on the current bytes in the window. Using these values, trigger points that indicate the boundaries of a block are identified. Sdhash \cite{chow_data_2010} extracts so-called features using a fixed-size sliding window that moves through the input. After calculating the shannon entropy for all extracted features, the lowest values within a defined interval are selected and hashed using SHA-1. The digest is split into five subhashes that are used to insert the respective feature into a bloom filter. As soon as a bloom filter reaches its capacity, a new one is initialized and used to store hashes. The final digest is the sequence of all bloom filters. Sdhash compares two objects by comparing each filter from the first object with every filter from the second object. Besides similarity detection, sdhash is also capable of detecting containment of certain sequences within objects. Instead of using digests for a similarity search directly, the authors in \cite{ludwig_friborg_malware_2019} employ \gls{lsh} for feature extraction in the context of malware classification. In this approach, similarity hashing algorithms are applied on javascript files that contain malware in form of obfuscated malicious code. The resulting hashes are used as input for a neural network that serves as a classifier. Within another work, \gls{lsh} is used for instance selection on an intrusion detection dataset \cite{baldini2021intrusion}. Instance selection identifies a subset of the original dataset whose application for the model training results in a similar or even increased performance as if the entire dataset had been used. An improved model performance can usually be achieved, when the reduction of samples results in a reduction of noise in the dataset. The authors identify that the main issue for the applicability of instance selection algorithms in \gls{ids} is the computational complexity of established algorithms. By incorporating an algorithm based on \gls{lsh} that is presented in \cite{aslani2020fast}, redundant samples are defined as samples whose similarity to a given instance exceeds a defined threshold. By removing redundant samples, the volume of the dataset is decreased significantly while the detection performance has remained the same or is even improved for specific attack classes. However, neither the authors in \cite{baldini2021intrusion} nor those in \cite{aslani2020fast} describe the construction of the hash function used to assign a bucket index to a sample. Motivated by the vast growth of malware samples, the authors in \cite{opricsa2014locality} present a fast algorithm for clustering malware into limited number of malware families, which are easier to manage than single samples. In particular, \gls{lsh} is employed for the clustering on a large collection of malware samples. Although the algorithm is still quadratic in theory, the authors state that the coeffcient for the quadratic term is several orders of magnitude smaller. The algorithm was tested on a collection of over one million malware samples, which is not further described.

\section{Generative Algorithms and Intrusion Detection}\label{sec:generative_algorithms_ids}

In the area of intrusion detection, generative algorithms primarily demonstrate their typical strengths, such that the ability to compensate for underrepresented classes with synthetic data generated by the generative model is by far the most frequent application. For example, the authors in \cite{8736331} present a network intrusion detection architecture in which a deep convolutional generative adversarial networks (DCGAN) is used to generate synthetic network intrusion data in order to balance the original training data. A multichannel SRU-based model is used as discriminative model for the classification task. The employed data for the experiments is versatile and includes the KDD99 dataset \cite{kdd99}, the NSL-KDD dataset \cite{nslkdd} and the CIC-IDS2017 dataset \cite{sharafaldin_toward_2018}. Evaluation results for both binary and multiclass classification show improvements regarding the employed metrics compared to the performance of common classification algorithms. However, the authors neglect to state parameter settings for the algorithms which were used for comparison and do not incorporate alternative upsampling algorithms in the experiments. In contrast to that, the authors in \cite{huang2020igan} compare their approach to common classification algorithms in combination with alternative balancing methods, namely random under-sampling, random over-sampling and SMOTE \cite{smote}. The presented architecture also employs a GAN for generating synthetic samples in the context of upsampling minority classes in the training data. Multiclass classification experiments were conducted using the NSL-KDD datset \cite{nslkdd}, the CIC-IDS2017 dataset \cite{sharafaldin_toward_2018} and the UNSW-NB15 dataset \cite{unswnb15}. Furthermore, the robustness of the approach is evaluated in detail by incrementally increasing the class imbalances in the experimental setup by random under-sampling. Although there is a slight downward trend in performance with increasing imbalance, the approach remains relatively stable. Another example is the architecture in \cite{lee2019ae}, where a feature extraction precedes the upsampling process by using an Autoencoder to transform statistical network flow features into a low-dimensional representation, which serve as input for the training of a CGAN. In all the stated approaches, the balancing of minority classes using generative algorithms leads to an increase in detection performance, compared to experiments without class balancing or with alternative balancing methods. However all these approaches do not incorporate an evaluation of the synthetic data before including them into the training data. The authors in \cite{shahriar2020g} present an interesting approach for evaluating synthetic data within their framework. There, the synthetic data is categorized into synthetic and pending data. Data flagged as synthetic is already verified and stays permanently in the datbase. The verification of the pending data depends on the decision of a controller module. Within the training phase, two classifier models are trained. The first model is trained on real and synthetic data and the second model is trained on real, synthetic and pending data. If the second model performs better than first model, the controller accepts the pending samples and flags them as synthetic. Otherwise the pending samples are rejected and removed from the database. Besides the utilization of generative algorithms for oversampling in intrusion detection, there are also other interesting applications. Some approaches employ GANs for synthesizing adversarial attack traffic that is used to evade an IDS, whose internal structure and parameters are unknown. For example, the authors in \cite{lin2022idsgan} state that the goal of the architecture is to generate malicious feature records similar to the attack traffic which can bypass the detection of the IDS. Then, attackers derive attack design principles for IDS evasion from the synthetic records.

From a technical perspective, in order to maintain specific functional features of attacks within the sample generation, specific attack attributes remain unaltered while nonfunctional features are fine-tuned in order to trick the classifier of the IDS. However, it has to be ensured that an adversarial perturbation does not invalidate the features used for intrusion detection. The approach presented in \cite{usama2019generative} also includes a training mechanism for defense purposes that increases the robustness of the IDS against adversarials. Specifically, an adversarial training \cite{szegedy2013intriguing} is conducted, in which the detection model learns the possible adversarial perturbations by training on clean and adversarial examples. Another application derives from the field of distributed learning of deep neural networks. The authors in \cite{ferdowsi2019generative} leverage a distributed-learning architecture in order to share attack information among multiple IDS without the requirement of sharing real data samples. In particular, a GAN is trained by deploying multiple discriminators at different entities that act as the detection unit. The feedback of the discriminators within the training phase is aggregated to a central generator model that incrementally creates a global perspective without exchanging real data. It is shown that each participating discriminator performs well in the evaluation and the central generator represents a global collection of local datasets. However, while in the training phase, the generator has to send synthetic data to each of the participating discriminators. Furthermore the appraoch is tested on a daily activity recognition dataset \cite{reyes2016transition}, which does not represent an intrusion detection dataset. Furthermore, besides malicious traffic also benign traffic of each local dataset is combined into the global knowledge which potentially introduces a significant amount of noise. 

\section{Main Distinguishing Features}

The presented approach achieves scalability and reduces overhead in two ways. First, \gls{lsh} is used to partition the input data in order to achieve data parallelism. By leveraging a cloud native implementation elastic scaling for serving bursty workloads is enabled. Second, the data volume is reduced significantly by representing it in the form of generative models before distributing it in the \gls{cids}. Exchanging model parameters only maintains data privacy. Sampling synthetic data from a generative model is considered as the decompression operation, enabling the full feature set instead of working on hashed data for privacy reasons. Furthermore, the approach does not implement a distributed P2P overlay but can rather be considered centralized from a logical perspective. However, a redundant and distributed implementation ensures scalability and availability. Data disseminatation is realized by synchronizing local and global data stores using an event-based application architecture. This means that updates on local data stores are sent first to the global data store, resulting in the output of events that are in turn distributed to local data stores. Upon the reception of an event, a local data store fetches the new global state. As events are sent via a messaging service, the consistency between different local data stores depends on the implemented messaging service. Consistency between data stores and their replicas depend on the implemented data store. Hence, data consistency can be considered differentiated on these two levels. The presented approach is not conform to the \gls{idmef}, because the information exchange is decoupled from the detection process. Each local intrusion detection dataset is enhanced using synthetic data from the exchanged generative models, including the advantage of data balancing. The respective intrusion detection is mainly independent and is only committed to a globally defined feature set.

\end{document}