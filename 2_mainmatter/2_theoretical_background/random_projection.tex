\section{Random Projection}

% introduction and applications

Random Projection (RP) is a technique with a wide range of applications, such as

clustering \cite{fern2003random}, 

Dimensionality reduction \cite{bingham2001random},

nearest neighbour search \cite{charikar2002similarity}, 

classification \cite{blum2005random}.

projection of multi-dimensional vectors onto a lower subspace using  a random matrix whose columns have unit lengths, that preserves the properties of the original space such as the distance and the angle between vectors and margins between classes. 
Allows to apply NN-search in the low-dim space; -> give the lemma in the structure

compared to pca:
 lower time complexity (approx. linear on average vs. (PCA time complexity))
 data-agnostic (no "interestingness", i.e. most variance, used for operations)
    - distributed computing (divide data into multiple partitions)
    - apply to streaming data (as no batches have to be used for operations)




% structuring of this section

\subsection{Dimensionality Reduction}
high dimensionality is a problem for unsupervised learning alogirithms
irrelevant and noisy features can mislead algorithm
high dim data may be sparse, such it is hard for algorithms to find any structure in the data
    two basic approaches: feature subsect selection \cite{dy2000feature} and feature transformations (projecting data onto subspaces of lower dimensionality than the original space \cite[374 ff.]{james2013introduction})

% strictly speaking RP is not a projection https://stats.stackexchange.com/questions/381991/is-random-projection-strictly-speaking-not-a-projection




\subsection{Locality Sensitive Hashing}
% name the connection to LSH
% motivation: retrieve similar items in O(1); the "algorithm"
% different types of LSH with different hashing families

% Johnson Lindenstrauss Lemma
%   - distance between points is approx. preserved