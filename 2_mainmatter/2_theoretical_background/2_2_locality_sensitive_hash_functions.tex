\subsection{Locality-Sensitive Hash Functions} \label{subsec:locality-sensitive-hashes}

Introduced by Indyck and Motwani in 1998 \cite{indyk_approximate_1998} as an algorithm that solves the approximate nearest neighbour problem (ANN), locality-sensitive hashing (LSH) has since been extensively researched and is now considered among the state of the art for approximate searches in high-dimensional spaces.\footnote{See \cite{nagarkar2021exploring} for an exhaustive survey of NN-Search Techniques.} The basic idea of the approach is to partition the input data using a hash function that is sensitive to the location of the input within the metric space. This way, similar inputs collide with a higher probability than inputs that are far apart. Thus, LSH exhibits fundamental differences to conventional hash functions \footnote{In the following, cryptographic and non-cryptograhpic hash functions are referred to as conventional hashing.}, although the most general definition applies to both.

A hash function is a function that maps a large input set to a smaller target set. The elements of the input set are called \textit{messages} or \textit{keys} and may be of arbitrary different lengths. The elements of the target set are called \textit{digests} or \textit{hash values} and are of fixed size length.

More specifically, we define a hash function as $h: A \rightarrow B : \bm{p} \mapsto \bm{u}$ where $A \subset \mathbb{R}^d$ with $d \in \mathbb{N}$ is the input set and $B=\{0, 1\}^k$ with $k \in \mathbb{N}$ the target set of all bit sequences of fixed size $k$, with $k < d$. According to the notation in \cite[256]{cormen2022introduction}, a hash table is defined by a hash function $h$, that maps the keyspace $K$ into the slots of a hash table $T[0 \,.\,.\, S-1], S \in \mathbb{N}$, i.e. $h: K \rightarrow \{0, 1, \dots, S-1\}$ with $S \ll |K|$. Thus, a message with key $k \in K$ hashes to slot $h(k)$. Additionally, $h(k)$ is the hash value of the key $k$.

Typically, conventional hashing is used for the realization of, e.g. hash tables, data integrity checks, error correction methods or database indexes. Depending on the application, different requirements are imposed on the utilized hash function. In this context, the most important property of a hash function is the probability of a \textit{collision}. A collision occurs when two keys $\bm{p}_1 \neq \bm{p}_2$ are projected onto the same hash value $\bm{u} = h(\bm{p}_1) = h(\bm{p}_2)$. For example, cryptographic hash functions are used in systems, where adversaries try to break such systems. Thus, different security requirements are defined for cryptograhpic hash functions \cite[349]{williamcryptography}. In particular, these hash functions are designed to be resistant against collisions, which is the key difference to LSH. Applications that do not require the hash function to be resistant against adversaries, e.g. hash tables, caches or de-duplication, are usually implemented by using a hash function that exhibits relaxed guarantees on the security properties in exchange for significant performance improvements. However, locality-sensitive hashes behave differently as illustrated in Figure \ref{fig:hashing_differences}. LSH projects similar inputs onto the same or near elements. In contrast, conventional hashing tries to distribute projections onto the elements of the target as randomly as possible.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{tikz/hashing_differences.pdf}
    \caption{The behaviour of conventional hashing and LSH}
    \label{fig:hashing_differences}
\end{figure}

As a locality-sensitive hashing function can be constructed as a general concept, specific families of functions can be derived. We refer to a \textit{family} of hash functions $\mathcal{H}: A \rightarrow B$ as a collection of hash functions that have the same domain and range, share a basic structure and are only differentiated by constants. Three basic requirements are demanded for such a family of functions \cite[99]{leskovec_rajaraman_ullman_2014}.

\begin{enumerate}
    \item Close pairs of input should be hashed in to the same bucket with higher probability than distant pairs.
    \item Specific functions of a family need to be statistically independent, such that the FPR and FNR can be improved by combining two or more functions.
    \item Functions need to be efficient, i.e., be faster compared to an exhaustive search.

\end{enumerate}

The first step is to define LSH generally. Applied on the $cR$-ANN, the first requirement states, with high probability, two points $p$ and $q$ should hash to the same hash value if their distance is at most $R$, i.e., $d(p,q) \leq R$. And if their distance is at least $cR$, the points should hash to different hash values, i.e. $d(p,q) > cR$. Thus,  A formal definition of a locality-sensitive hash function is given as follows \cite{andoni2006near}.

\begin{definition}[Locality-Sesitive Hash Function]
    Given a target distance $R \in \mathbb{R}, R>0$, an approximation factor $c \in \mathbb{R}, c>1$ and probability thresholds $P_1, P_2 \in \mathbb{R}$, a family $\mathcal{H} = \{h: A \rightarrow B$\} is called $(R, cR, P_1, P_2)$-sensitive if for any two points $p,q \in A$ and any hash function $h$ chosen uniformly at random from $\mathcal{H}$ the following conditions are satisfied:
        \[
        \setlength\arraycolsep{0pt}
        \renewcommand\arraystretch{1.2}
            \begin{array}{LCL}
                d(p,q) \leq R & \implies & P[h(p)=h(q)] \geq P_1, \\
                d(p,q) \geq cR & \implies & P[h(p)=h(q)] \leq P_2.
            \end{array}
        \]
\end{definition}
 
Ideally, the gap between $P_1$ and $P_2$ should be as big as possible as depicted in Figure \ref{subfig:exact_probability}, which in fact represents an exact search. This is, as already discussed, no option due to its time and space requirements. Considering a single locality-sensitive function as shown in Figure \ref{subfig:lsh_probability}, where the probability gap between $P_1$ and $P_2$ is relatively close, the false negative rate would be relatively high. Increasing the gap would require to increase $c$ and lead to a high number of false positives. Therefore, a single function would provide only a tradeoff. But it is possible to increase $P_1$ close to $1$ and decrease $P_2$ close to $1/n$ while keeping $R$ and $cR$ fixed as shown in Figure \ref{subfig:lsh_probability_boosted} by introducing a process called \textit{amplification}. Two different forms, namely the AND-construction and the OR-construction, can be applied.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tikz/lsh_probability.pdf}
        \caption{Single LSH Function.}
        \label{subfig:lsh_probability}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tikz/lsh_probability_boosted.pdf}
        \caption{Amplified LSH Function.}
        \label{subfig:lsh_probability_boosted}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \includegraphics[width=\textwidth]{tikz/exact_probability.pdf}
            \caption{Exact Search}
            \label{subfig:exact_probability}
    \end{subfigure}
    \caption{The behaviour of a $(R, cR, P_1, P_2)$-sensitive function in (\ref{sub@subfig:lsh_probability}) and (\ref{sub@subfig:lsh_probability_boosted}) (adapted from \cite[100]{leskovec_rajaraman_ullman_2014}) approaching the ideal probability gap in (\ref{sub@subfig:exact_probability}) resembling the behaviour of an exact search.}
    \label{fig:lsh_probability}
\end{figure}
   
By applying a logical AND-construction on $\mathcal{H}$ the threshold $P_2$ is reduced. For that, sample $K$ hash functions indepedently from $\mathcal{H}$ and hash each point $p \in P$ to a $K$-dimensional vector with a new constructed function $g \in \mathcal{H}^K$ as

\begin{equation}\label{eq:or_construction}
    g(p) = [h_1(p), h_2(p), \dots, h_K(p)].
\end{equation}

Since all hash functions $\{h_1, \dots, h_K\} \in \mathcal{H}$ are statistically independent the product rule applies and for any two points $p$ and $q$, a collision occurs if and only if $h_i(p)=h_i(q)$ for all $k \in \{1, \dots, K\}$. The probability gap is then defined as follows:

\begin{align}
    P[h_i(p)=h_i(q)] \geq P_1 \implies P[g(p)=g(q)] \geq P_1^K \\
    P[h_i(p)=h_i(q)] \leq P_2 \implies P[g(p)=g(q)] \leq P_2^K.
\end{align}

Thus, by increasing $K$ the threshold $P_2$ can be arbitrarily decreased and approaches zero. However, the AND-construction lowers both $P_1$ and $P_2$. In order to improve $P_1$, the OR-construction is introduced. For that, a number of $L \in \mathbb{N}$ functions $\{g_1, \dots, g_L\}$ are constructed, where each function $g_l, \; l \in \{1, \dots, L\}$ stems from a different family $\mathcal{H}_l$. Note, that the algorithm is successful, when the two points $p, q$ collide at least once for some $g_l$. Therefore, hashing a point $p \in P$ with each $g_l$ results in the following probability for collision.

\begin{align}
    P[\exists l, g_l(p)=g_l(q)] &= 1 - P[\forall l, g_l(p) \neq g_l(q)] \\
                                &= 1 - P[g_l(p) \neq g_l(q)]^L \\
                                &\geq 1 - (1-P_1^K)^L
\end{align}

As the AND-construction lowers both $P_1$ and $P_2$, similarly the OR-construction rises both thresholds. By choosing $K$ and $L$ judiciously, $P_2$ approaches zero while $P_1$ approaches one. Both constructions may be concatenated in any order to manipulate $P_1$ and $P_2$. Of course, the more constructions are used and the higher the values for the parameters $K$ and $L$ are picked, the more time is considered for the application of the function. 

The general algorithm for solving the $cR$-ANN using LSH consists of two consecutive steps. First, a set of points is required for the construction of the data structrue as described in Algorithm \ref{alg:lsh_preprocess}. For each function $g_l$ a corresponding hash table $T_l$ is initialized that will store all data points $p_n$. In other words, all data points are preprocessed and stored $L$ times.The resulting data structure represents the database, which is searched through, when answering a query. Note, that a common collision handling method, such as seperate chaining for example, is applied if a certain slot is already occupied. 

\begin{algorithm}
    \caption{LSH Preprocessing}
    \label{alg:lsh_preprocess}
    \algsetup{indent=2em}
    \begin{algorithmic}[1]
        \REQUIRE $N$ points $p_n \in P$ with $n \in \{1, \dots, N\}, N \in \mathbb{N}$
        \ENSURE data structure $\{T_1, \dots, T_L\}$
        \STATE construct hash functions $g_1, \dots, g_L$ each of length $K$ (see Equation \ref{eq:or_construction})
        \FORALL{$g_l \in \{g_1, \dots, g_L\}$}
            \STATE $T_l \leftarrow$ new HashTable
            \FORALL{$p_n \in P$}
                \STATE add $p_n$ to $T_l[g_l(p_n)]$
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
 \end{algorithm}

 Answering a query point $q$ is done by hashing it multiple times for each $g_l$ as in the preprocessing step (see Algorithm \ref{alg:lsh_query}). Each time, a set of points $P$, stored in the correspoding hash table $T_l$ at the slot $g_l(q)$, are retrieved. That is, identify all $p_n$, such that $g_l(p_n) = g_l(q)$. For each identified $p_n$, a distance function evaluates if the distance $d(p_n, q)$ is within the search perimeter $cR$. If positive, the point is added to the result set $S$, which is returned at the end of the algorithm.

 \begin{algorithm}
    \caption{LSH Query}
    \label{alg:lsh_query}
    \algsetup{indent=2em}
    \begin{algorithmic}[1]
        \REQUIRE a query point $q$ and data structure $\{T_1, \dots, T_L\}$
        \ENSURE a set $S$ of nearest neighours of $q$
        \STATE $S \leftarrow \emptyset$
        \FORALL{$g_l \in \{g_1, \dots, g_L\}$}
            \STATE $P \leftarrow T_l[g_l(q)]$
            \IF{$|P| > 0$}
                \FORALL{$p_n \in P$}
                    \IF{$d(p_n,q) \leq cR$}
                        \STATE add $p_n$ to $S$
                    \ENDIF
                \ENDFOR
            \ENDIF
        \ENDFOR
        \RETURN $S$
    \end{algorithmic}
 \end{algorithm}

 With regard to preprocessing, the consideration of space complexity of Algorithm \ref{alg:lsh_preprocess} is interesting, whereas the runtime of Algorithm \ref{alg:lsh_query} is the most relevant when examining the execution of queries. Fortunately, lower bounds on both quantities have been proven in \cite{motwani2006lower}, resulting in the following theorem.

\begin{theorem}
    Let $(X,d)$ be a metric on a subset of $\mathbb{R}^d$. Given a $(R, cR, P_1, P_2)$-locality sensitive hash family $\mathcal{H}$ and write $\rho = \frac{\text{log}(1/P_1)}{\text{log}(1/P_2)}$. Then for $n = |X|$ and for any $n \geq \frac{1}{P_2}$ there exists a solution to the $cR$-ANN with space complexity $O(dn+n^{1+\rho})$ and query time of $O(n^{\rho})$.
\end{theorem}

Therefore, it follows that the query time of LSH can only be sublinear, if $\rho < 1$, which is the case if the inequality $P_1 > P_2$ is satisfied.

