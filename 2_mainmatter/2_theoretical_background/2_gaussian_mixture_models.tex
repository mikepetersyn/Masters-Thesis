\section{Gaussian Mixture Models}

\subsection{Gaussian Distribution}

\subsection{Density Estimation}

\subsection{Gaussian Mixtures}

How many components should be chosen for fittin a GMM? A common method is to select the model $M$ with the highest probability given the data $D$. Assuming that the model $M$ is completely described by its set of parameters $\theta$, the maximum likelihood function of the model $M$ is given by 

\begin{equation}
    \hat{L} = p(D|\hat{\theta}, M),
\end{equation}
    
answers the question of what is the probability that $D$ is explained by $M$. Note, that the carat denotes the parameters that maximize the probability, i.e., the maximum likelihood function. Theoretically, in the case of a $GMM$, one could just increase the number of parameters $K$, e.g., the number of components, arbitrarily until the $K = N$ in order to obtain the maximum likelihood. In practice, this would not only lead to a bad runtime of the algorithm, but also overfit the model. Thus, the maximum likelihood of the model $L$ has to be balanced against the number of model parameters $K$. The Bayesian information criterion (BIC) is considered as a standard criterion for model selection of GMM because of its theoretical consistency in choosing the number of components \cite{keribin2000consistent}.

In general, the BIC can be defined as

\begin{equation}
    \text{BIC} = K \, \text{ln}(N) - 2 \, \text{ln}(\hat{L}),
\end{equation}

which derives from the findings in \cite{schwarz1978estimating}. The BIC balances the number of model parameters $K$ and number of data points $N$ against the maximum likelihood function $L$. In the model selection, the optimal number of model parameters $K$ minimizes the BIC, such that the BIC provides a principled way of selecting between multiple different models. More complex models almost always fit the data better, resulting in a lower value of $-2 \, \text{ln}(\hat{L})$. The BIC penalizes extra parameters by introducing the term $K \, \text{ln}(N)$. Beyond penalizing more parameters, it furthermore assists in making a judgement as to how the additional parameters improve the model in the presence of more data.

Considering a mixture model with $K$ components defined by 

\begin{equation}
    \begin{aligned}
        &p(\bm{x}|\theta) = \sum\limits_{k=1}^K \pi_k \mathcal{N}(x | \theta), \\
        &0 \leq \pi_k \leq 1, \sum\limits_{k=1}^K \pi_k = 1, \\
        &\theta := \{\overline{\bm{x}}_k, \bm{\mathrm{C}}_k, \pi_k : k = 1, \dots, K \},
    \end{aligned}
\end{equation}

the parameters of $K$ multivariate normal distributions are to learn, with $d$ dimensions, these are $d$ values in the mean vector.  Since a covariance matrix is symmetric, only $d(d+1)/2$ entries in a full covariance matrix have to be computed. Additionaly, $K$ mixture weights have to be determined. Since these sum to one, it is sufficient to determine $K-1$ weights, leading to $Kd + K(d(d+1)/2)+K-1$ parameters. Thus, the BIC for a dataset $D$ with $N$ datapoints of dimensionality $d$ and a GMM $M$ as defined above is stated as 

\begin{equation}
    \text{BIC}(M|D) = (Kd + K(d(d+1)/2)+K-1) \, \text{ln}(N) - 2 \, \text{ln}(\hat{L}).
\end{equation}

A model selection algorithm could look like the following.

\begin{algorithm}
    \caption{GMM Selection with BIC}
    \label{alg:gmm_selection_bic}
    \algsetup{indent=2em}
 
    \begin{algorithmic}[1]
        \REQUIRE Dataset $D$
        \ENSURE Gaussian Mixture Model $M$
        \STATE $B \leftarrow$ new Array
        \STATE GMM $\leftarrow$ new Array
        \FORALL{$k$ in $\{1, \dots, K\}$}
            \STATE $M_k \leftarrow \text{fitGMM}(D)$
            \STATE $b_k \leftarrow \text{BIC}(M|D)$
            \STATE GMM$[k] \leftarrow M_k$
            \STATE $B[k] \leftarrow b_k$
        \ENDFOR
        \STATE $\hat{k} \leftarrow \text{argmin}(B)$
        \STATE $\hat{M} \leftarrow \text{GMM}[\hat{k}]$
        \RETURN $\hat{M}$

        
    \end{algorithmic}
 \end{algorithm}