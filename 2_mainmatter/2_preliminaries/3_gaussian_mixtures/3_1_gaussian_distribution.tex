\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{The Gaussian Distribution}\label{subsec:gaussian_distribution}

The Gaussian normal distribution forms the basic building block for \acrshort{gmm}s. Depending on the observed random variable, different types of normal distributions exist. In particular, the multivariate normal distribution is focused on when considering statistical flow features, which are typically continuous and high dimensional. In the following, different definitions for the normal distribution are introduced and important properties are presented that are essential for calculations during the learning phase of \acrshort{gmm}s.

Let be a multivariate random variable $\bm{X} = (X_1, \dots, X_D)^\top$, where each element $X_d$ with $d \in \{1, \dots, D\}, D \in \mathbb{N}$ is a univariate random variable that follows a normal distribution, which is referred to as $X_d \sim \mathcal{N}(\mu, \sigma^2)$. In \cite[175]{dei_2020}, the univariate and multivariate Gaussian is defined as follows.

\begin{definition}[Univariate Gaussian Distribution]
A continuous, univariate random variable $X$ is said to follow a normal distribution, if it exhibits a probability density function
\begin{equation*}
p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} exp\bigg(- \frac{(x-\mu)^2}{2\sigma^2}\bigg),
\end{equation*}
\end{definition}

with $x \in \mathbb{R}$ and where $\mu$ refers to the mean and $\sigma^2$ is the variance of the distribution. Then, if $\bm{X}$ follows a normal distribution, this is expressed as a multivariate Gaussian distribution, denoted by $\bm{X} \sim \mathcal{N}_D(\bm{\mu}, \bm{\Sigma})$, which is fully described by its mean vector $\bm{\mu}$ and its covariance matrix $\bm{\Sigma}$.

\newpage

\begin{definition}[Multivariate Gaussian Distribution]\label{def:multivariate_pdf}
A multivariate random variable $\bm{X}$ follows a normal distribution, if it is described by a probability density function

\begin{equation*}
    p(\bm{x} | \bm{\mu}, \bm{\Sigma}) = (2\pi)^{-\frac{D}{2}} |\bm{\Sigma}|^{-\frac{1}{2}}exp\left(-\frac{1}{2}(\bm{x}-\bm{\mu})^T \bm{\Sigma}^{-1}(\bm{x}-\bm{\mu})\right),
\end{equation*}
\end{definition}

with $\bm{x} \in \mathbb{R}^D$. The mean vector $\bm{\mu}$ specifies the estimate of the expected value of the distribution, with each of its components describing the mean $\mu$ of the corresponding dimension. The empirical covariance $\bm{\Sigma}$ models the estimate of the variance along each dimension as well as the correlation between the different dimensions. The diagonal elements of $\bm{\Sigma}$ describe the variance of the random variable corresponding to the respective dimension and the off-diagonal elements describe the covariance relationship between the respective random variables. An illustration of the influence of the described parameters on the location and shape of a multivariate distribution is given in Figure \ref{fig:multiple_bivariate}. Specifically, three bivariate normal distributions are considered, whose random variables each have different values with respect to their correlation, while the mean vector is the same for all distributions.
 
\begin{figure}%
    \centering%
    \begin{subfigure}[b]{0.25\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/contour_plot_neg.tex}%
        \caption{}%
        \label{subfig:bivariate_neg}%
    \end{subfigure}%
    % \hfill%
    \begin{subfigure}[b]{0.33\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/contour_plot.tex}%
        \caption{}%
        \label{subfig:bivariate_neutral}%
    \end{subfigure}%
    % \hfill%
    \begin{subfigure}[b]{0.33\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/contour_plot_pos.tex}%
        \caption{}%
        \label{subfig:bivariate_pos}%
    \end{subfigure}%
    \caption[Bivariate Gaussian distributions with different correlations]{Bivariate gaussian distributions exhibit different shapes with a changing correlation value between the random variables $x_1$ and $x_2$: (\subref{subfig:bivariate_neg}) negative, (\subref{subfig:bivariate_neutral}) zero and (\subref{subfig:bivariate_pos}) positive correlation.}%
    \label{fig:multiple_bivariate}%
\end{figure}%

Having introduced the basic definition of Gaussian distributions, it is now examined how they can be manipulated to obtain information necessary for parameter learning of Gaussian mixture models. Two practical algebraic properties of normal distributions, namely closure under \textit{conditioning} and \textit{marginalization} are detailed for this purpose. Being closed under conditioning and marginalization in this case means that when one or more components of a Gaussian distribution is marginalized out or conditioned on, that the resulting distribution is still Gaussian. This not only distinguishes Gaussian distributions from other distributions, but also makes them easier to handle mathematically. Since both marginalization and conditioning act on subsets of the original distribution, the following notation is introduced first. Considering a multivariate Gaussian random variable $\bm{X} \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$ with $\bm{X}_M \in \mathbf{R}^M$, where $M < N$, and $\bm{X}_N \in \mathbf{R}^N$, where $N=D-M$, we partition $\bm{X}$ according to 

\begin{equation*}
    \bm{X} = \left[ \begin{array}{c}
        \bm{X}_M \\
        \bm{X}_N
    \end{array} \right].
\end{equation*}
\newpage
In general, $\bm{X}_M$ is chosen to be the first $M$ elements of $\bm{X}$ and $\bm{X}_N$ the rest. Accordingly, $\bm{\mu}$ and $\bm{\Sigma}$ are partitioned as
\begin{equation*}
    \left[ \begin{array}{c}
        \bm{X}_M \\
        \bm{X}_N
    \end{array} \right] \sim \mathcal{N} \left( \left[\begin{array}{c}
         \bm{\mu}_M \\
         \bm{\mu}_N
    \end{array}\right], \left[\begin{array}{cc}
        \bm{\Sigma}_{MM} & \bm{\Sigma}_{MN} \\
        \bm{\Sigma}_{NM} & \bm{\Sigma}_{NN}
    \end{array}\right] \right).
\end{equation*}

With this form, it is now possible to express the extraction of partial information from multivariate probability distributions by means of marginalization. In \cite[177]{dei_2020}, the marginal of a Gaussian is defined as follows.

\begin{definition}[Marginal of a Gaussian Distribution]\label{def:marg_gaussian} 
Given $\bm{X} \sim \mathcal{N}_D(\bm{\mu}, \bm{\Sigma})$, with partitions $\bm{X}_M, \bm{X}_N$, the distributions of $\bm{X}_M$ and $\bm{X}_N$ are called marginals and their corresponding probability density function can be obtained by
\begin{equation*}
    p(\bm{x}_M) = \int p(\bm{x}_M,\bm{x}_N)d\bm{x}_N.
\end{equation*}
\end{definition}

Furthermore, each partition $\bm{X}_M, \bm{X}_N$ only depend on its corresponding entries in $\bm{\mu}$ and  $\bm{\Sigma}$. The authors in \cite[177]{dei_2020} state that the marginal of a Gaussian is also a Gaussian, which is determined by

\begin{equation*}
    \bm{X}_M \sim \mathcal{N}(\bm{\mu}_M, \bm{\Sigma}_{MM}).
\end{equation*}

The conditional Gaussian is typically utilized in the context of posterior distributions, such as in the process of density estimation of \glspl{gmm}, and is defined in \cite[p. 177]{dei_2020} as follows.

\begin{definition}[Conditional of a Gaussian Distribution] 
Given $\bm{X} \sim \mathcal{N}_D(\bm{\mu}, \bm{\Sigma})$, with partitions $\bm{X}_M, \bm{X}_N$, the conditional distribution is defined as

\begin{align*}
    p(\bm{x}_M | \bm{x}_N) &= \mathcal{N}(\bm{\mu}_{M | N}, \bm{\Sigma}_{M | N}), \\
    \bm{\mu}_{M | N} &= \bm{\mu}_M + \bm{\Sigma}_{M N} + \bm{\Sigma}^{-1}_{N N}(\bm{x}_N - \bm{\mu}_N),\\
    \bm{\Sigma}_{M | N} &= \bm{\Sigma}_{M M} - \bm{\Sigma}_{M N} \bm{\Sigma}^{-1}_{N N}\bm{\Sigma}_{N M}.\\
\end{align*}
\end{definition}


Accordingly, the conditional distribution of a Gaussian distribution is also a Gaussian \cite[p. 177]{dei_2020}, which is defined by 

\begin{equation*}
    \bm{X}_{M | N} \sim \mathcal{N}(\bm{\mu}_{M | N}, \bm{\Sigma}_{M | N}).
\end{equation*}


A line of arguments proving the stated theorems can be found in section 2.3 in \cite{bis_2006}. An example for marginal and conditional Gaussians can be found in Figure \ref{fig:marginal_conditional_gaussian}. The functions of $p(x_M)$ and $p(x_N)$ are plotted on the side grids.

\begin{figure}[t!]
    \centering
    \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/marginal}
    \caption[Marginals of a Gaussian]{Marginals $p(x_M)$ and $p(x_N)$.}
    \label{fig:marginal_conditional_gaussian}
\end{figure}


\end{document}