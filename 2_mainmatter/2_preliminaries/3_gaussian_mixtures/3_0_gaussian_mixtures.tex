\documentclass[../../../main.tex]{subfiles}
\begin{document}
\section{Gaussian Mixtures}

\Glsplural{gmm} refer to a density estimation technique the the machine learning context. With the help of density estimation techniques, large amounts of data can be compactly represented in a model. An estimate of important characteristics of an unobservable probability density function, such as mean or variance, is constructed based on the observed data. The density function is considered as the distribution according to which the observed data population is distributed. In the context of this work, \glspl{gmm} are used for the compact representation of attack data. Instead of exchanging the datasets directly in the \gls{cids}, the data is instead represented by gmms, whereupon their parameters are shared. In this way, the data volume of the \gls{cids} communication is significantly reduced. The \gls{gmm} can be considered as a generative model, since new data can be generated by ancestral sampling. Thus, by reconstructing the \glsplural{gmm} from the shared parameters, the generation of synthetic data in the respective local infrastructures can be enabled. This data can in turn be used flexibly in the context of improving the respective intrusion detection.

For that purpose, importanct basics and concepts are introduced in this section. First, the Gaussian normal distribution is presented in Section~\ref{subsec:gaussian_distribution} as the \gls{gmm} is based on that parametric family. Then, in Section~\ref{subsec:gaussian_mixture_model} mixture models in general and \glspl{gmm} in particular are defined. Lastly, in Section \ref{subsec:em_algorithm} the \acrshort{gmm} is interpreted in terms of a \gls{lvm} and the \gls{ema} is presented, which performs the computation of model parameters of latent variable models in an iterative scheme.

\subfile{3_1_gaussian_distribution}

\subfile{3_2_gaussian_mixture_model}

\subfile{3_3_em_algorithm}

\end{document}