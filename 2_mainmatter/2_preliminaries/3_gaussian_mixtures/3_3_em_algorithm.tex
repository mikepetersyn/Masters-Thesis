\documentclass[../../../main.tex]{subfiles}
\begin{document}

\subsection{The Expectation Maximization Algorithm} \label{subsec:em_algorithm}
So far, with the definition and the example, an intuitive explanation of the \gls{gmm} has been given. Before it is explained how to estimate the parameters of the model, it is necessary to look at the subject from the perspective of probabilistic modeling. Probabilistic models, in general, utilize the mathematics of probability theory in order to express all forms of uncertainty and noise that is associated with the learning task. If a Bayesian interpretation is thereby applied to the probabilities, then this is commonly referred to as Bayesian Inference. This allows, for example, the estimation of the parameters of a probability distribution or a statistical model utilizing Bayes' Theorem \cite[p.245]{dei_2020}. In this case, discrete latent variables must be introduced into the construction, which allows a probabilistic model to be formed by defining a joint distribution over observed variables and latent variables \cite[432]{bis_2006}. 

The term latent variable usually refers to a variable that cannot be observed directly, but can be inferred from other observable variables using mathematical models \cite{bor_2003}. Models involving such latent variables, called \gls{lvm}, are usually harder to fit than models without latent variables, but often have fewer parameters due to their natural implication of a bottleneck, resulting in a compressed representation of the data \cite[337]{mur_2012}. 

For simplicity, first a single data point $\bm{x}$ is considered, which is later expanded to a set of data points $\mathcal{X}:=\{\bm{x}_1, \dots, \bm{x}_N\}, \mathcal{X} \subset \mathbb{R}^D$. It is further assumed a \acrshort{gmm} with $K$ components. Thus, a $K$-dimensional binary random variable $\bm{z}$ is introduced.

Note that $\bm{z} = [z_1, \dots, z_K]^\top$ indicates whether the $k$th mixture component is responsible for generating the data point $\bm{x}$ and that a data point can only be generated by one mixture component. Hence, only a particular element $z_k$ is equal to one and all other elements are equal to zero, such that

\begin{equation}
    z_k \in \{0,1\}, \quad \sum\limits^K_{k=1} z_k=1.
\end{equation}

\begin{figure}[b!]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/plate_single_point}
        \caption{}
        \label{subfig:plate_single_point}
    \end{subfigure}
    % \hspace*{1cm}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/plate_multi_point}
        \caption{}
        \label{subfig:plate_multi_point}
        \end{subfigure}
        \caption[A Gaussian Mixture Model represented in plate notation]{
            Both graphs represent a \gls{gmm} using plate notation. While the left graph (\subref{subfig:plate_single_point}) includes a single data point $\bm{x}$, the graph on the right side (\subref{subfig:plate_multi_point}) extends the model to a full dataset with $N$ i.i.d. data points $\{\bm{x}_n\}$ and corresponding latent variables $\{\bm{z}_n\}$. Note, that the directed edge from the latent variable to the observed data reflects the joint distribution from (\ref{eq:joint_lat_var}).}
         \label{fig:plate_notation}
\end{figure}

For the construction of a probabilistic model, it is necessary to specify the joint distribution of the observed data $\bm{x}$ and the latent variable $\bm{z}$. Therefore, the factors of the joint distribution are defined as

\begin{equation}\label{eq:joint_lat_var}
    p(\bm{x},\bm{z}) = p(\bm{z})p(\bm{x}|\bm{z}).
\end{equation}

The responsibility of mixture component $k$ generating a data point $\bm{x}$ can be expressed as the conditional distribution of $\bm{x}$ given a specific assignment for $\bm{z}$, which is a Gaussian

\begin{equation}\label{eq:cond_lat_var}
    p(\bm{x} | z_k=1) = \mathcal{N}(\bm{x} | \bm{\mu}_k, \bm{\Sigma}_k) \quad \Rightarrow \quad p(\bm{x}|\bm{z}) = \prod\limits^K_{k=1}\mathcal{N}(\bm{x} | \bm{\mu}_k, \bm{\Sigma}_k)^{z_k} .
\end{equation}

In practice, there exists no knowledge about the value assignment of the latent variables. Therefore, a prior is set to $\bm{z}$, which is defined as

\begin{equation}\label{eq:prior_lat_var}
    p(\bm{z})=\bm{\pi}=[\pi_{1}, \dots, \pi_{K}]^\top, \quad \sum\limits_{k=1}^K\pi_{k}=1.
\end{equation}

Now, there is a way to describe the \textit{probability} that the $k$th mixture component generated the data point $\bm{x}$ as
\begin{equation}
    p(z_k=1)=\pi_k.
\end{equation}



Then the conditional distribution $p(\bm{x}|\bm{z})$ from (\ref{eq:cond_lat_var}) and the prior distribution $p(\bm{z})$ from (\ref{eq:prior_lat_var}) are plugged into the joint distribution $p(\bm{z})p(\bm{x}|\bm{z})$ from (\ref{eq:joint_lat_var}) and the marginal distribution $p(\bm{x})$ is obtained by summing the joint distribution over all possible states of $\bm{z}$, which results in 

\begin{equation}\label{eq:joint_marg_lat_var}
    p(\bm{x}) = \sum\limits_{\bm{z}}p(\bm{z})p(\bm{x}|\bm{z}) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(\bm{x} | \bm{\mu}, \bm{\Sigma}).
\end{equation}

It can be summarized that the \gls{gmm} has been extended by including the discrete latent variable  $\bm{z}$ and that the result is still consistent with the previous definition, since the marginal distribution $p(\bm{x})$ is a Gaussian mixture of the form (\ref{eq:gmm_def}). This enables the joint distribution $p(\bm{x}, \bm{z})$ to be used in place of the marginal distribution $p(\bm{x})$, greatly simplifying parameter estimation by applying the \gls{ema}.

So far, it was assumed that the dataset consists of a single data point $\bm{x}$. Now, the dataset is extended to $N$ data points. From that, it follows that every data point $\bm{x}_n$ possesses its own latent variable $\bm{z}_n$. All latent variables can be summarized by $\mathcal{Z} \subset \mathbb{R}^K$. This extension to a full dataset is also illustrated comparatively in Figure \ref{fig:plate_notation}. In accordance to that, the corresponding posterior probability after observing $\bm{x}$, referred to as $r_{nk}$, is obtained using Bayes' Theorem as

\begin{equation}\label{eq:responsibilities}
    \begin{aligned}
        r_{nk}=p(z_{nk}=1|\bm{x}_n) &= \frac{p(z_{nk}=1)p(\bm{x} | z_k=1)}{p(\bm{x})}\\[5pt]
        &= \frac{p(\bm{x}_n) | z_{nk}=1)p(z_{nk}=1)}{\sum_{j=1}^K\pi_j p(\bm{x}_n | z_{nj}=1)p(z_{nj}=1)} \\[5pt]
        &= \frac{\pi_k\mathcal{N}(\bm{x}_n) | \bm{\mu}_k, \bm{\Sigma}_k)}{\sum_{j=1}^K\pi_j\mathcal{N}(\bm{x}_n) | \bm{\mu}_j, \bm{\Sigma}_j)}.
    \end{aligned}
\end{equation}

The posterior $r_{nk}$ can be interpreted, from the perspective of a generative model, as the proportion with which a component is involved in the generation of the point $\bm{x}_n$.

The central question is how to fit the set of unknown parameters $\bm{\theta}$ to a given set of data $\mathcal{X}$ and therefore finding a good approximation for the unknown distribution $p(x)$. For estimation problems based on i.i.d. data points, the \gls{mle} is an applicable method \cite[p. 317]{dei_2020}. By doing this, the likelihood of each data point $\bm{x} \in \mathcal{X}$ given a specific parametrization $\bm{\theta}$ is maximized. To do so, the likelihood function, and the log-likelihood function respectively, must first be constructed, where each individual likelihood term is a Gaussian mixture density as

\begin{equation*}
    \begin{aligned}
        p(\mathcal{X}|\theta) &= \prod\limits_{n=1}^Np(\bm{x}_n|\bm{\theta}), \quad p(\bm{x}_n|\bm{\theta})=\sum\limits^K_{k=1}\pi_k\mathcal{N}(\bm{x}_n| \bm{\mu}, \bm{\Sigma}),\\[5pt]
        \mathrm{log} \, p(\mathcal{X}|\bm{\theta}) &= \sum\limits_{n=1}^N \mathrm{log} \, p(\bm{x}_n|\bm{\theta}) = \underbrace{ \sum\limits_{n=1}^N \mathrm{log} \sum\limits_{k=1}^K \pi_k \mathcal{N}(\bm{x}_n | \bm{\mu}, \bm{\Sigma})}_{=:\mathcal{L}}.
    \end{aligned}
\end{equation*}

The common solution scheme that would follow would be to calculate the gradient $\partial{\mathcal{L}}/\partial{\bm{\theta}}$, set it to zero and solve for $\bm{\theta}$. Unfortunately, there is no closed form solution for a \gls{mle} in this form, because the summation over the $K$ components prevents the logarithm from being applied to the Gaussian densities within \cite[p. 435]{bis_2006}. That means that each of the partial derivatives of the model parameters depends on all $K$ parameters of the GMM through $r_{nk}$, which prohibits a closed form solution. 

However, a solution can be found using the \gls{ema}. The key idea here is to update one model parameter at a time while keeping the others fixed. For that, the ML estimators for the individual parameters of the \gls{gmm} are required first, which will be applied in the \gls{ema} in the following step. The following necessary conditions are established:

\begin{equation*}
    \begin{aligned}
    &\frac{\partial{\mathcal{L}}}{\partial{\bm{\mu}_k}} = \bm{0}^T \; &\Longleftrightarrow \quad &\sum\limits_{n=1}^N\frac{\partial{\mathrm{log}p(\bm{x}_n| \theta)}}{\partial{\bm{\mu}_k}} = \bm{0}^\top\\[5pt]
    &\frac{\partial{\mathcal{L}}}{\partial{\bm{\Sigma}_k}} = \bm{0} \; &\Longleftrightarrow \quad &\sum\limits_{n=1}^N\frac{\partial{\mathrm{log}p(\bm{x}_n| \theta)}}{\partial{\bm{\Sigma}_k}} = \bm{0}\\[5pt]
    &\frac{\partial{\mathcal{L}}}{\partial{\pi_k}} = 0 \; &\Longleftrightarrow \quad &\sum\limits_{n=1}^N\frac{\partial{\mathrm{log}p(\bm{x}_n| \theta)}}{\partial{\pi_k}} = 0.\\
    \end{aligned}
\end{equation*}

In the following only the partial derivatives of the model parameters that will be used in the \acrshort{ema} are presented. A detailed calculation can be found in \cite[pp.319]{dei_2020}.

\begin{equation}\label{eq:ml_estimator}
    \begin{aligned}
    &\bm{\mu}_k' &= \quad &\frac{\sum_{n=1}^N r_{nk}\bm{x}_n}{\sum_{n=1}^N r_{nk}} \\
    &\bm{\Sigma}_k' &= \quad &\frac{1}{N_k}\sum\limits_{n=1}^N r_{nk}(\bm{x}_n-\bm{\mu}_k)(\bm{x}_n-\bm{\mu}_k)^\top\\
    &\pi_k' &= \quad &\frac{N_k}{N}
    \end{aligned}
\end{equation}

% \subsubsection{Expectation Maximization Algorithm} \label{ch:Expectation_Maximization_Algorithm}

The \acrshort{ema} is an iterative procedure that starts with an initial estimate of the parameters, which are updated incrementally until convergence is detected. The initial values for the parameters can either be chosen randomly or obtained with an heuristic method, e.g. by using k-means to cluster the data first and subsequently defining the weights based on the resulting k-means memberships \cite[pp. 325]{dei_2020}. 

Each iteration $t$ of the \acrshort{ema} consists mainly of two steps, the Expectation step and the Maximization step. A third step is subsequently needed for the calculation of the convergence criterion. First, in the expectation step, the posterior distribution $r_{nk}$ from (\ref{eq:responsibilities}) is calculated for all data points $\bm{x}_n$ with the current set of parameters $\bm{\theta}^{(t)}$. Then, the posterior from the previous step is used in the maximization step for computing the new parameter values with the Maximum-Likelihood estimators defined in (\ref{eq:ml_estimator}). At the end of each iteration, the log likelihood for the new set of parameters $\bm{\theta}^{(t+1)}$ and its change from the log likelihood from the previous iteration is computed. Then, the algorithm can test for convergence by comparing $\Delta \mathcal{L}$ with the convergence threshold $\epsilon$ and the algorithm stops if no significant change occur anymore.

% % \begin{algorithm}[H]
% % \setstretch{1.75}
% % \SetAlgoLined
% % \textbf{Initialize:} \;
% % $\qquad t=0$\;
% % $\qquad \theta^{(t)} := \{\overline{\bm{x}}_k, \bm{\mathrm{C}}_k, \pi_k : k = 1, \dots, K \}$\;
% % $\qquad \mathcal{L}^{(t)} = \mathrm{log}p(\bm{X}|\theta^{(t)})$\;
% %  \While{$\Delta \mathcal{L} > \epsilon$ }{
% %  \BlankLine
% % 1. \textit{Expectation-Step:}
% % \BlankLine
% %     $\qquad r_{nk} = \frac{\pi_k\mathcal{N}(\bm{x}_n | \overline{\bm{x}}_k, \bm{\mathrm{C}}_k)}{\sum_{j=1}^K\pi_j\mathcal{N}(\bm{x}_n) | \overline{\bm{x}}_j, \bm{\mathrm{C}}_j)} \text{ with } \theta^{(t)}$ \;
% % \BlankLine
% % 2. \textit{Maximization-Step:}
% % \BlankLine
% % $\qquad \pi_k^{(t+1)} \quad = \frac{N_k}{N}$\;
% % $\qquad \overline{\bm{x}}_k^{(t+1)} = \quad \frac{\sum_{n=1}^N r_{nk}\bm{x}_n}{\sum_{n=1}^N r_{nk}}$\;
% % $\qquad \bm{\mathrm{C}}_k^{(t+1)} = \quad \frac{1}{N_k}\sum_{n=1}^N r_{nk}(\bm{x}_n-\overline{\bm{x}}_k)(\bm{x}_n-\overline{\bm{x}}_k)^T$\;
% % \BlankLine
% % 3. \textit{Convergence Terms:}\;
% % $\qquad \mathcal{L}^{(t+1)} = \mathrm{log}p(\bm{X}|\theta^{(t+1)})$\;
% % $\qquad \Delta \mathcal{L} = | \mathcal{L}^{(t+1)} - \mathcal{L}^{(t)} |$\;
% % $\qquad t = t+1$\;
% % \BlankLine
% %  }
% %  \caption{Expectation Maximization for Gaussian Mixture Models}
% % \end{algorithm}

% Figure \ref{fig:em_algo_gmm} illustrates the EM algorithm for fitting a \acrshort{gmm} with three components to the two-dimensional dataset from Figure \ref{fig:multimodal_dataset}. The resulting \acrshort{gmm} with its component parameters can also be seen in example \ref{ex:gmm_examples}. Each component is represented by an coloured ellipse, that is formed by its mean and covariance matrix. Every single data point $\bm{x}_n$ is associated with an probability vector, that describes each component's responsibility for creating $\bm{x}_n$. This is visualized by coloring each data point according to its probability vector, i.e. a mixed proportion of purple, cyan and orange. At the beginning, the three component parameters are chosen randomly and no posterior $r_{nk}$ is computed yet and thus no data point is colored. With each iteration, the parameters approach the optimum, which they reach after the 21st iteration, and the algorithm converges.

% % \begin{figure}[h!]
% %     \centering
% %     \includegraphics[width=1\textwidth]{figures/em_gaussian.pdf}
% %     \caption{Illustration of the EM algorithm for fitting a GMM with three components on a two-dimensional dataset.}
% %     \label{fig:em_algo_gmm}
% % \end{figure}

% How many components should be chosen for fittin a GMM? A common method is to select the model $M$ with the highest probability given the data $D$. Assuming that the model $M$ is completely described by its set of parameters $\theta$, the maximum likelihood function of the model $M$ is given by 

% \begin{equation}
%     \hat{L} = p(D|\hat{\theta}, M),
% \end{equation}
    
% answers the question of what is the probability that $D$ is explained by $M$. Note, that the carat denotes the parameters that maximize the probability, i.e., the maximum likelihood function. Theoretically, in the case of a $GMM$, one could just increase the number of parameters $K$, e.g., the number of components, arbitrarily until the $K = N$ in order to obtain the maximum likelihood. In practice, this would not only lead to a bad runtime of the algorithm, but also overfit the model. Thus, the maximum likelihood of the model $L$ has to be balanced against the number of model parameters $K$. The Bayesian information criterion (BIC) is considered as a standard criterion for model selection of GMM because of its theoretical consistency in choosing the number of components \cite{keribin2000consistent}.

% In general, the BIC can be defined as

% \begin{equation}
%     \text{BIC} = K \, \text{ln}(N) - 2 \, \text{ln}(\hat{L}),
% \end{equation}

% which derives from the findings in \cite{schwarz1978estimating}. The BIC balances the number of model parameters $K$ and number of data points $N$ against the maximum likelihood function $L$. In the model selection, the optimal number of model parameters $K$ minimizes the BIC, such that the BIC provides a principled way of selecting between multiple different models. More complex models almost always fit the data better, resulting in a lower value of $-2 \, \text{ln}(\hat{L})$. The BIC penalizes extra parameters by introducing the term $K \, \text{ln}(N)$. Beyond penalizing more parameters, it furthermore assists in making a judgement as to how the additional parameters improve the model in the presence of more data.

% Considering a mixture model with $K$ components defined by 

% \begin{equation}
%     \begin{aligned}
%         &p(\bm{x}|\theta) = \sum\limits_{k=1}^K \pi_k \mathcal{N}(x | \theta), \\
%         &0 \leq \pi_k \leq 1, \sum\limits_{k=1}^K \pi_k = 1, \\
%         &\theta := \{\overline{\bm{x}}_k, \bm{\mathrm{C}}_k, \pi_k : k = 1, \dots, K \},
%     \end{aligned}
% \end{equation}

% the parameters of $K$ multivariate normal distributions are to learn, with $d$ dimensions, these are $d$ values in the mean vector.  Since a covariance matrix is symmetric, only $d(d+1)/2$ entries in a full covariance matrix have to be computed. Additionaly, $K$ mixture weights have to be determined. Since these sum to one, it is sufficient to determine $K-1$ weights, leading to $Kd + K(d(d+1)/2)+K-1$ parameters. Thus, the BIC for a dataset $D$ with $N$ datapoints of dimensionality $d$ and a GMM $M$ as defined above is stated as 

% \begin{equation}\label{eq:bic}
%     \text{BIC}(M|D) = (Kd + K(d(d+1)/2)+K-1) \, \text{ln}(N) - 2 \, \text{ln}(\hat{L}).
% \end{equation}

% A model selection algorithm could look like the following.

% \begin{algorithm}
%     \caption{GMM Selection with BIC}
%     \label{alg:gmm_selection_bic}
%     \algsetup{indent=2em}
 
%     \begin{algorithmic}[1]
%         \REQUIRE Dataset $D$
%         \ENSURE Gaussian Mixture Model $M$
%         \STATE $B \leftarrow$ new Array
%         \STATE GMM $\leftarrow$ new Array
%         \FORALL{$k$ in $\{1, \dots, K\}$}
%             \STATE $M_k \leftarrow \text{fitGMM}(D)$
%             \STATE $b_k \leftarrow \text{BIC}(M|D)$
%             \STATE GMM$[k] \leftarrow M_k$
%             \STATE $B[k] \leftarrow b_k$
%         \ENDFOR
%         \STATE $\hat{k} \leftarrow \text{argmin}(B)$
%         \STATE $\hat{M} \leftarrow \text{GMM}[\hat{k}]$
%         \RETURN $\hat{M}$

        
%     \end{algorithmic}
%  \end{algorithm}

\end{document}