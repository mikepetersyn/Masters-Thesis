\documentclass[../../../main.tex]{subfiles}
\begin{document}

\subsection{The Expectation Maximization Algorithm} \label{subsec:em_algorithm}
So far, with the definition and the example, an intuitive explanation of the \gls{gmm} has been given. Before it is explained how to estimate the parameters of the model, it is necessary to look at the subject from the perspective of probabilistic modeling. Probabilistic models, in general, utilize the mathematics of probability theory in order to express all forms of uncertainty and noise that is associated with the learning task. If a Bayesian interpretation is thereby applied to the probabilities, then this is commonly referred to as Bayesian Inference. This allows, for example, the estimation of the parameters of a probability distribution or a statistical model utilizing Bayes' Theorem \cite[p.245]{dei_2020}. In this case, discrete latent variables must be introduced into the construction, which allows a probabilistic model to be formed by defining a joint distribution over observed variables and latent variables \cite[432]{bis_2006}. 

The term latent variable usually refers to a variable that cannot be observed directly, but can be inferred from other observable variables using mathematical models \cite{bor_2003}. Models involving such latent variables, called \gls{lvm}, are usually harder to fit than models without latent variables, but often have fewer parameters due to their natural implication of a bottleneck, resulting in a compressed representation of the data \cite[337]{mur_2012}. 

For simplicity, first a single data point $\bm{x}$ is considered, which is later expanded to a set of data points $\mathcal{X}:=\{\bm{x}_1, \dots, \bm{x}_N\}, \mathcal{X} \subset \mathbb{R}^D$. It is further assumed a \acrshort{gmm} with $K$ components. Thus, a $K$-dimensional binary random variable $\bm{z}$ is introduced.

Note that $\bm{z} = [z_1, \dots, z_K]^\top$ indicates whether the $k$th mixture component is responsible for generating the data point $\bm{x}$ and that a data point can only be generated by one mixture component. Hence, only a particular element $z_k$ is equal to one and all other elements are equal to zero, such that

\begin{equation}
    z_k \in \{0,1\}, \quad \sum\limits^K_{k=1} z_k=1.
\end{equation}

\begin{figure}[b!]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/plate_single_point}
        \caption{}
        \label{subfig:plate_single_point}
    \end{subfigure}
    % \hspace*{1cm}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/plate_multi_point}
        \caption{}
        \label{subfig:plate_multi_point}
        \end{subfigure}
        \caption[A Gaussian Mixture Model represented in plate notation]{
            Both graphs represent a \gls{gmm} using plate notation. While the left graph (\subref{subfig:plate_single_point}) includes a single data point $\bm{x}$, the graph on the right side (\subref{subfig:plate_multi_point}) extends the model to a full dataset with $N$ i.i.d. data points $\{\bm{x}_n\}$ and corresponding latent variables $\{\bm{z}_n\}$. Note, that the directed edge from the latent variable to the observed data reflects the joint distribution from (\ref{eq:joint_lat_var}).}
         \label{fig:plate_notation}
\end{figure}

For the construction of a probabilistic model, it is necessary to specify the joint distribution of the observed data $\bm{x}$ and the latent variable $\bm{z}$. Therefore, the factors of the joint distribution are defined as

\begin{equation}\label{eq:joint_lat_var}
    p(\bm{x},\bm{z}) = p(\bm{z})p(\bm{x}|\bm{z}).
\end{equation}

The responsibility of mixture component $k$ generating a data point $\bm{x}$ can be expressed as the conditional distribution of $\bm{x}$ given a specific assignment for $\bm{z}$, which is a Gaussian

\begin{equation}\label{eq:cond_lat_var}
    p(\bm{x} | z_k=1) = \mathcal{N}(\bm{x} | \bm{\mu}_k, \bm{\Sigma}_k) \quad \Rightarrow \quad p(\bm{x}|\bm{z}) = \prod\limits^K_{k=1}\mathcal{N}(\bm{x} | \bm{\mu}_k, \bm{\Sigma}_k)^{z_k} .
\end{equation}

In practice, there exists no knowledge about the value assignment of the latent variables. Therefore, a prior is set to $\bm{z}$, which is defined as

\begin{equation}\label{eq:prior_lat_var}
    p(\bm{z})=\bm{\pi}=[\pi_{1}, \dots, \pi_{K}]^\top, \quad \sum\limits_{k=1}^K\pi_{k}=1.
\end{equation}

Now, there is a way to describe the \textit{probability} that the $k$th mixture component generated the data point $\bm{x}$ as
\begin{equation}
    p(z_k=1)=\pi_k.
\end{equation}



Then the conditional distribution $p(\bm{x}|\bm{z})$ from (\ref{eq:cond_lat_var}) and the prior distribution $p(\bm{z})$ from (\ref{eq:prior_lat_var}) are plugged into the joint distribution $p(\bm{z})p(\bm{x}|\bm{z})$ from (\ref{eq:joint_lat_var}) and the marginal distribution $p(\bm{x})$ is obtained by summing the joint distribution over all possible states of $\bm{z}$, which results in 

\begin{equation}\label{eq:joint_marg_lat_var}
    p(\bm{x}) = \sum\limits_{\bm{z}}p(\bm{z})p(\bm{x}|\bm{z}) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(\bm{x} | \bm{\mu}, \bm{\Sigma}).
\end{equation}

It can be summarized that the \gls{gmm} has been extended by including the discrete latent variable  $\bm{z}$ and that the result is still consistent with the previous definition, since the marginal distribution $p(\bm{x})$ is a Gaussian mixture of the form (\ref{eq:gmm_def}). This enables the joint distribution $p(\bm{x}, \bm{z})$ to be used in place of the marginal distribution $p(\bm{x})$, greatly simplifying parameter estimation by applying the \gls{ema}.

So far, it was assumed that the dataset consists of a single data point $\bm{x}$. Now, the dataset is extended to $N$ data points. From that, it follows that every data point $\bm{x}_n$ possesses its own latent variable $\bm{z}_n$. All latent variables can be summarized by $\mathcal{Z} \subset \mathbb{R}^K$. This extension to a full dataset is also illustrated comparatively in Figure \ref{fig:plate_notation}. In accordance to that, the corresponding posterior probability after observing $\bm{x}$, referred to as $r_{nk}$, is obtained using Bayes' Theorem as

\begin{equation}\label{eq:responsibilities}
    \begin{aligned}
        r_{nk}=p(z_{nk}=1|\bm{x}_n) &= \frac{p(z_{nk}=1)p(\bm{x} | z_k=1)}{p(\bm{x})}\\[5pt]
        &= \frac{p(\bm{x}_n) | z_{nk}=1)p(z_{nk}=1)}{\sum_{j=1}^K\pi_j p(\bm{x}_n | z_{nj}=1)p(z_{nj}=1)} \\[5pt]
        &= \frac{\pi_k\mathcal{N}(\bm{x}_n) | \bm{\mu}_k, \bm{\Sigma}_k)}{\sum_{j=1}^K\pi_j\mathcal{N}(\bm{x}_n) | \bm{\mu}_j, \bm{\Sigma}_j)}.
    \end{aligned}
\end{equation}

The posterior $r_{nk}$ can be interpreted, from the perspective of a generative model, as the proportion with which a component is involved in the generation of the point $\bm{x}_n$.

The central question is how to fit the set of unknown parameters $\bm{\theta}$ to a given set of data $\mathcal{X}$ and therefore finding a good approximation for the unknown distribution $p(x)$. For estimation problems based on i.i.d. data points, the \gls{mle} is an applicable method \cite[p. 317]{dei_2020}. By doing this, the likelihood of each data point $\bm{x} \in \mathcal{X}$ given a specific parametrization $\bm{\theta}$ is maximized. To do so, the likelihood function, and the log-likelihood function respectively, must first be constructed, where each individual likelihood term is a Gaussian mixture density as

\begin{equation*}
    \begin{aligned}
        p(\mathcal{X}|\theta) &= \prod\limits_{n=1}^Np(\bm{x}_n|\bm{\theta}), \quad p(\bm{x}_n|\bm{\theta})=\sum\limits^K_{k=1}\pi_k\mathcal{N}(\bm{x}_n| \bm{\mu}, \bm{\Sigma}),\\[5pt]
        \mathrm{log} \, p(\mathcal{X}|\bm{\theta}) &= \sum\limits_{n=1}^N \mathrm{log} \, p(\bm{x}_n|\bm{\theta}) = \underbrace{ \sum\limits_{n=1}^N \mathrm{log} \sum\limits_{k=1}^K \pi_k \mathcal{N}(\bm{x}_n | \bm{\mu}, \bm{\Sigma})}_{=:\mathcal{L}}.
    \end{aligned}
\end{equation*}

The common solution scheme that would follow would be to calculate the gradient $\partial{\mathcal{L}}/\partial{\bm{\theta}}$, set it to zero and solve for $\bm{\theta}$. Unfortunately, there is no closed form solution for a \gls{mle} in this form, because the summation over the $K$ components prevents the logarithm from being applied to the Gaussian densities within \cite[p. 435]{bis_2006}. That means that each of the partial derivatives of the model parameters depends on all $K$ parameters of the GMM through $r_{nk}$, which prohibits a closed form solution. 

However, a solution can be found using the \gls{ema}. The key idea here is to update one model parameter at a time while keeping the others fixed. For that, the ML estimators for the individual parameters of the \gls{gmm} are required first, which will be applied in the \gls{ema} in the following step. The following necessary conditions are established:

\begin{equation*}
    \begin{aligned}
    &\frac{\partial{\mathcal{L}}}{\partial{\bm{\mu}_k}} = \bm{0}^T \; &\Longleftrightarrow \quad &\sum\limits_{n=1}^N\frac{\partial{\mathrm{log}p(\bm{x}_n| \theta)}}{\partial{\bm{\mu}_k}} = \bm{0}^\top\\[5pt]
    &\frac{\partial{\mathcal{L}}}{\partial{\bm{\Sigma}_k}} = \bm{0} \; &\Longleftrightarrow \quad &\sum\limits_{n=1}^N\frac{\partial{\mathrm{log}p(\bm{x}_n| \theta)}}{\partial{\bm{\Sigma}_k}} = \bm{0}\\[5pt]
    &\frac{\partial{\mathcal{L}}}{\partial{\pi_k}} = 0 \; &\Longleftrightarrow \quad &\sum\limits_{n=1}^N\frac{\partial{\mathrm{log}p(\bm{x}_n| \theta)}}{\partial{\pi_k}} = 0.\\
    \end{aligned}
\end{equation*}

In the following only the partial derivatives of the model parameters that will be used in the \acrshort{ema} are presented. A detailed calculation can be found in \cite[pp.319]{dei_2020}.

\begin{equation}\label{eq:ml_estimator}
    \begin{aligned}
    &\bm{\mu}_k' &= \quad &\frac{\sum_{n=1}^N r_{nk}\bm{x}_n}{\sum_{n=1}^N r_{nk}} \\
    &\bm{\Sigma}_k' &= \quad &\frac{1}{N_k}\sum_{n=1}^N r_{nk}(\bm{x}_n-\bm{\mu}_k)(\bm{x}_n-\bm{\mu}_k)^\top\\
    &\pi_k' &= \quad &\frac{N_k}{N}
    \end{aligned}
\end{equation}

% \subsubsection{Expectation Maximization Algorithm} \label{ch:Expectation_Maximization_Algorithm}

The \acrshort{ema} is an iterative procedure that starts with an initial estimate of the parameters, which are updated incrementally until convergence is detected. The initial values for the parameters can either be chosen randomly or obtained with an heuristic method, e.g. by using k-means to cluster the data first and subsequently defining the weights based on the resulting k-means memberships \cite[pp. 325]{dei_2020}. 

Each iteration $t$ of the \acrshort{ema} consists mainly of two steps, the Expectation step and the Maximization step. A third step is subsequently needed for the calculation of the convergence criterion. First, in the expectation step, the posterior distribution $r_{nk}$ from (\ref{eq:responsibilities}) is calculated for all data points $\bm{x}_n$ with the current set of parameters $\bm{\theta}^{(t)}$. Then, the posterior from the previous step is used in the maximization step for computing the new parameter values with the Maximum-Likelihood estimators defined in (\ref{eq:ml_estimator}). At the end of each iteration, the log likelihood for the new set of parameters $\bm{\theta}^{(t+1)}$ and its change from the log likelihood from the previous iteration is computed. Then, the algorithm can test for convergence by comparing $\Delta \mathcal{L}$ with the convergence threshold $\epsilon$ and the algorithm stops if no significant change occur anymore.

\begin{algorithm}
    \setstretch{1.5}
    \caption{\gls{ema} for \gls{gmm}}
    \label{alg:ema_gmm}
    % \algsetup{indent=2em}
    \begin{algorithmic}[1]
        \STATEx \textbf{Initialize:}
        \STATE $\qquad t = 0$
        \STATE $\qquad \bm{\theta}^{(t)} := \{ \bm{\mu}_k, \bm{\Sigma}_k, \pi_k \, | \, k=1,\dots,K \}$
        \STATE $\qquad \mathcal{L}^{(t)} = \text{log} \, p(\mathcal{X} \, | \, \bm{\theta}^{(t)})$

        \WHILE{$\Delta \mathcal{L} > \epsilon$}
            \STATEx $\quad$ 1. Expectation-Step:
            \STATE $\qquad r_{nk} = \frac{\pi_k \mathcal{N}(\bm{x}_n \, | \, \bm{\mu}_k, \bm{\Sigma}_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\bm{x}_n \, | \, \bm{\mu}_j, \bm{\Sigma}_j)} \text{ with } \bm{\theta}^{(t)}$

            \STATEx $\quad$ 2. Maximization-Step:
            \STATE $\qquad \pi_k^{(t+1)} = \frac{N_k}{N}$
            \STATE $\qquad \bm{\mu}^{(t+1)} = \frac{\sum_{n=1}^N r_{nk} \bm{x}_n}{\sum_{n=1}^N r_{nk}}$
            \STATE $\qquad \bm{\Sigma}_k^{(t+1)} = \frac{1}{N_k}\sum_{n=1}^N r_{nk}(\bm{x}_n-\bm{\mu}_k)(\bm{x}_n-\bm{\mu}_k)^\top$

            \STATEx $\quad$ Convergence Terms:
            \STATE $\qquad \mathcal{L}^{(t+1)} = \text{log} \, p(\mathcal{X} \, | \, \bm{\theta}^{(t+1)})$
            \STATE $\qquad \Delta \mathcal{L} = | \mathcal{L}^{(t+1)} - \mathcal{L}^{(t)} |$
            \STATE $\qquad t = t+1$

        \ENDWHILE
    \end{algorithmic}
 \end{algorithm}

 \begin{figure}[b!]
    \centering
    \includestandalone{2_mainmatter/2_preliminaries/3_gaussian_mixtures/tikz/em_dataset}
    \caption{Illustration of the EM algorithm for fitting a GMM with three components on a two-dimensional dataset.}
    \label{fig:em_algo_gmm}
\end{figure}

Figure \ref{fig:em_algo_gmm} illustrates the \gls{ema} for fitting a \gls{gmm} with three components on a two-dimensional synthetic dataset with three classes. Each component of the \gls{gmm} is represented by an coloured ellipse, that is formed by its mean and covariance matrix. Every single data point $\bm{x}_n$ is associated with an probability vector, that describes each component's responsibility for creating $\bm{x}_n$. A data point is assigned to the component that exhibits the highest responsibility value. This is visualized by coloring each data point according to their assignment to a component. In this example, the three component parameters are chosen randomly and no posterior $r_{nk}$ is computed in the beginning $(t=0)$ and thus no data point is colored. In this state, the true class labels can bee seen in different grayscales. 

With each iteration, the parameters approach the local optimum, which they reach after iteration $21$ $(t=21)$, and the algorithm converges. In this toy example, the three classes can be seperated well by three components. Note that multiple components may describe a single class as well. This raises the question of how many components should be ideally chosen for fitting a \gls{gmm}, when the number of classes are not known beforehand as in this example? 

A common method is to select the model with the highest probability given the data $\mathcal{X}$. Recall that the likelihood $\mathcal{L}$ answers the question of what is the probability that $\mathcal{X}$ is explained by the model. Theoretically, in the case of a \gls{gmm}, one could just arbitrarily increase the number of components $K$ until $K = N$ in order to obtain the maximum likelihood. In practice, this would not only lead to a bad runtime of the algorithm, but also overfit the model. Thus, the maximum likelihood of the model $\mathcal{L}$ has to be balanced against the number of model parameters. The \gls{bic} is considered as a standard criterion for \gls{gmm} model selection because of its theoretical consistency in choosing the number of components \cite{keribin2000consistent}. Let $|\bm{\theta}|$ be the number of model parameters in general, then the \gls{bic} is defined as
\begin{equation}
    \text{BIC} = |\bm{\theta}| \, \text{ln} \, (N) - 2 \, \text{ln} \, (\mathcal{L}),
\end{equation}

which derives from the findings in \cite{schwarz1978estimating}. The BIC balances the number of model parameters $|\bm{\theta}|$ and number of data points $N$ against the likelihood function $\mathcal{L}$. In the model selection, the optimal number of model parameters $|\bm{\theta}|$ minimizes the \gls{bic}, such that it provides a principled way of selecting between multiple different models. More complex models almost always fit the data better, resulting in a lower value for the term $-2 \, \text{ln}(\mathcal{L})$. The \gls{bic} penalizes extra parameters by introducing the term $|\bm{\theta}| \, \text{ln} \, (N)$. A model selection utilizing the \gls{bic} is described in Algorithm~\ref{alg:gmm_selection_bic}, with $\bm{\theta}$ being the set of parameter settings for \gls{gmm}.

\begin{algorithm}
    \caption{GMM Selection with BIC}
    \label{alg:gmm_selection_bic}
    \begin{algorithmic}[1]
        \REQUIRE Dataset $\mathcal{X}$
        \ENSURE Gaussian Mixture Model \gls{gmm}
        \STATE $L_{\text{GMM}} \leftarrow$ new List
        \FORALL{$\theta$ in $\bm{\theta}$}
            \STATE $\text{GMM}_k \leftarrow \text{fitGaussianMixture}(\mathcal{X}, \theta)$
            \STATE $\text{BIC}_k \leftarrow \text{computeBIC}(\text{GMM}|\mathcal{X})$
            \STATE append $(\text{GMM}_k, \text{BIC}_k)$ to $L_{\text{GMM}}$
        \ENDFOR
        \STATE sort $L_{\text{GMM}}$ ascending by $\text{BIC}_k$ 
        \STATE \gls{gmm} $\leftarrow L_{\text{GMM}}[0]$ \COMMENT{get head of the list}
        \STATE \textbf{return} \gls{gmm} 
    \end{algorithmic}
 \end{algorithm}

To be more precisely, the parameters of $K$ multivariate normal distributions are to learn. With $D = \text{dim}(\mathcal{X})$, these are $D$ values in the mean vector. Since a covariance matrix is symmetric, only $D(D+1)/2$ entries in a full covariance matrix have to be computed. Additionaly, $K$ mixture weights have to be determined, leading to $Kd + K(d(d+1)/2)+K$ parameters. Thus, the \gls{bic} for a dataset $\mathcal{X}$ with $N$ datapoints of dimensionality $D$ and a \gls{gmm} as defined above is stated as 
\begin{equation}\label{eq:bic}
    \text{BIC}(\text{GMM}|\mathcal{X}) = (KD + K(D(D+1)/2)+K) \, \text{ln}(N) - 2 \, \text{ln} \, (\mathcal{L}).
\end{equation}

\end{document}