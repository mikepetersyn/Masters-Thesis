\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{The Gaussian Mixture Model}

In this section, the \acrshort{gmm} is presented as a model for density estimation. For this purpose, at the beginning a short motivation for the application of the presented model will be given. Then, mixture models in general and \acrshort{gmm} in particular are defined. After that, two main concepts will be explained. First, the \acrshort{gmm} is interpreted in terms of a \acrshort{lvm}. Second, the \acrshort{ema} is presented, which performs the computation of model parameters of latent variable models in an iterative scheme.

\subsubsection{Motivation}
Using statistical estimation techniques, it is possible to estimate an unobservable underlying probability density function from observed data. This allows data to be compactly represented with a density from a parametric family, such as a Gaussian distribution. However, all conventional parametric distributions are limited in their modeling capabilities when confronted with real data. For example, looking at the marginal distribution $p(x_1)$ in Figure \ref{fig:multimodal_dataset}, it is apparent that the data follow a multimodal distribution, i.e., have more than one center. A density estimate using a simple Gaussian distribution is not sufficient to effectively represent such multimodal data. Therefore, a more flexible type of distribution needs to be introduced that can be used for density estimation.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/gaussian_example_dataset.pdf}
%     \caption[Illustration of an exemplary dataset with two dimensions exhibiting a multimodal distribution]{This figure illustrates an examplary dataset with two dimensions. Within the center of this figure, there is a scatter plot showing the observed data points $\bm{x} \in \bm{X}$. At the bottom and right edge of the scatter plot, there are histograms showing the marginal distributions of the dataset.}
%     \label{fig:multimodal_dataset}
% \end{figure}

\subsubsection{Definition}

The idea is to represent a multimodal distribution by constructing a linear combination of multiple simple distributions, each of which representing a unimodal sub-population of the data, which is formalized under the term \textit{mixture model} \cite[p.111]{bis_2006}.

\begin{definition}[Mixture Model] \cite[p. 315]{dei_2020}
A mixture model is a linear combination of $K$ parametric distributions $p_k$, each weighted by a mixture weight $\pi_k$, with the following form 
\begin{align*}
    &p(x) = \sum\limits_{k=1}^K \pi_k p_k (x), \\
    &0 \leq \pi_k \leq 1, \sum\limits_{k=1}^K \pi_k = 1.
\end{align*}
\end{definition}

A distribution $p_k$ within this model is called mixture component and the sum of the mixture weights equals to 1, such that the probability density of the mixture components equals to 1 as well. 

Mixture Models can use any arbitrary parametric distribution as component density, but the most common mixture model is the Gaussian mixture model, using Gaussians as components \cite[214]{has_2009}. First, \acrshort{gmm}s utilize the practical mathematical properties of Gaussians, introduced earlier in Section \ref{sec:gaussian_distribution}. Furthermore, the quality of the model, in terms of its ability to estimate real data distributions, is theoretically supported by the Central Limit Theorem, which states, that most data distributions converge to a normal distribution on average as the number of data points increases (see \cite[p.222]{jay_2003} for a proof).

\begin{theorem}[Central Limit Theorem]\label{th:central_limit} \cite[p.241]{montgomery_2010}
Consider $N$ i.i.d. random variables $X_i$ with $\mathrm{E}[X_i]=\overline{x}$ and $\mathrm{var}[X_i]=\sigma^2$ and let $S_N=\sum^N_{i=1}X_i$. It can be shown that, as $N$ increases, that
\begin{equation*}
    p(S_N) \sim \mathcal{N}\left(\overline{x}, \frac{\sigma^2}{N}\right).
\end{equation*}
\end{theorem}

\begin{definition}[Gaussian Mixture Model]\label{def:gmm} \cite[p. 315]{dei_2020}
A \acrlong{gmm} is a combination of a finite number of $K$ Gaussian distributions $\mathcal{N}(\bm{x}|\theta_k)$ which is fully described by a probability density function $p$ and its parameter set $\theta$ as
\begin{equation}\label{eq:gmm_def}
    \begin{aligned}
        &p(\bm{x}|\theta) = \sum\limits_{k=1}^K \pi_k \mathcal{N}(x | \theta), \\
        &0 \leq \pi_k \leq 1, \sum\limits_{k=1}^K \pi_k = 1, \\
        &\theta := \{\overline{\bm{x}}_k, \bm{\mathrm{C}}_k, \pi_k : k = 1, \dots, K \}.
    \end{aligned}
\end{equation}
\end{definition}

% \begin{figure}[h!]
% \begin{example}{Gaussian Mixture Model with three components}{gmm_examples}

% This example illustrates a \acrshort{gmm} with three components that has been fitted to the dataset shown in Figure \ref{fig:multimodal_dataset}.

% \begin{equation*}
%         p(\bm{x}|\theta) = \sum\limits_{k=1}^{3}\pi_k\mathcal{N}(x|\overline{\bm{x}}_k, \bm{\mathrm{C}}_k)
% \end{equation*}

% \begin{equation*}
% \setlength{\jot}{10pt}
%     \begin{aligned}
%     k = 1:& \quad \pi_1=0.23, \; p_1(\bm{x})=\mathcal{N} \left( \left[\begin{array}{c}
%          3.00 \\
%         -4.68
%     \end{array}\right], \left[\begin{array}{cc}
%         0.57 & -0.41 \\
%         -0.41 & 0.66
%     \end{array}\right] \right)\\
%     k = 2:& \quad  \pi_2=0.33 \; p_2(\bm{x})=\mathcal{N} \left( \left[\begin{array}{c}
%          0.30 \\
%         -6.34
%     \end{array}\right], \left[\begin{array}{cc}
%         0.75 & 0.68 \\
%         0.68 & 1.45
%     \end{array}\right] \right)\\
%     k = 3:& \quad \pi_3=0.44 \; p_3(\bm{x})=\mathcal{N} \left( \left[\begin{array}{c}
%          5.36 \\
%         -5.29
%     \end{array}\right], \left[\begin{array}{cc}
%         0.67 & 0.31 \\
%         0.31 & 0.85
%     \end{array}\right] \right)\\
%     \end{aligned}
% \end{equation*}

%     \begin{subfigure}[ht]{0.66\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/example_dataset_gmm_3d.png}
%         \caption{}
%         \label{fig:gmm_ex_joint}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[ht]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/example_dataset_contour_gmm.png}
%         \caption{}
%         \label{fig:gmm_ex_contour}
%     \end{subfigure}
%     \label{fig:contour_gaussian}
%     \caption[Plots of a Distribution according to a Gaussian Mixture Model]{Both plots visualize a Gaussian Mixture Model that has been fitted to the exemplary dataset from Figure \ref{fig:multimodal_dataset}. Unlike a simple distribution, the mixture model is capable of representing multimodal data.}

% \end{example}
% \end{figure}


\end{document}