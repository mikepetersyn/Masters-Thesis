\documentclass[../../../main.tex]{subfiles}
\begin{document}

\subsection{Signal and Noise}

In general, dimensionality reduction methods make use of the following idea. Measuring data is error prone and associated with both incompleteness and redundancy. This trend increases with the number of measurement points, i.e. dimensions. Thus, many dimensions in multivariate data can be explained by a combination of other dimensions. This underlying structure can be exploited by partitioning the full space into subspaces of signal and noise \cite[217]{sco_2015}. Figure \ref{fig:pca_motivation} shows two randomly created datasets. The plot on the right displays a negative correlation and the direction that carries the largest portion of the signal can be located visually. In this two-dimensional example, the direction that is perpendicular to that is the direction that carries the least amount of signal and is referred to as noise. The goal is to discard a significant amount of noise in the data and work with a more compact representation, with losing as little signal as possible.

% \begin{figure}[h!t]
%     \centering
    
%     \begin{subfigure}[ht]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/pca_motivation_two_dim_uncorrelated.pdf}
%         \caption{}
%         \label{fig:pca_motivation_uncorrelated}
%     \end{subfigure}
% \hfill
%     \begin{subfigure}[ht]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/pca_motivation_two_dim_correlated.pdf}
%         \caption{}
%         \label{fig:pca_motivation_correlated}
%     \end{subfigure}
    
%     \caption[Two randomly created datasets illustrating the difference in signal and noise]{Two randomly created datasets. In \ref{fig:pca_motivation_uncorrelated} the data is not correlated, i.e. exhibits low redundancy in terms of data measurement. The data in \ref{fig:pca_motivation_correlated} features a negative correlation, which is an indicator of redundant information. In other words, the data could be meaningfully represented by a single dimension, without losing much of the information.}
%     \label{fig:pca_motivation}
% \end{figure}


\subsubsection{Problem Setting}

All data points $\bm{x}_n$ can be represented as a linear combination of the orthonormal basis. Considering an example dataset with three dimensions, the orthonormal basis of $\mathbb{R}^3$ is the canonical basis $e_1=[1,0,0]^T, e_2 = [0,1,0]^T, e_3=[0,0,1]^T$. 

The central question associated with \acrshort{pca} is whether there is another basis, which is a \textit{linear} combination of the original basis, that best represents the data. From this perspective, \acrshort{pca} can be considered a change-of-basis problem. Recalling the example from Figure \ref{fig:pca_motivation}, a better representation of the data is related to a lower amount of noise and redundancy in the data. Further, by assuming linearity, the problem is simplified by restricting the set of potential bases and enabling an efficient solution with matrix decomposition techniques \cite{shl_2014}.

It is not always the case that the correlation among variables is not of interest. But in the case of \acrshort{pca}, correlation represents redundancy, which is desired to be minimized.


\subsubsection{Assumptions and Solution Approach}

To derive a solution approach, some assumptions are made beforehand, which are proven in the following chapter. It is assumed that data containing more redundancy generally have lower variance and vice versa. Therefore, the dynamics of interest exist in the directions of highest variance. It is further assumed that the canonical basis in which the data was recorded is a representation that has inherent redundancies and noise. For this reason, a suitable basis is searched that represents a linear transformation of the canonical basis, such that the base aligns with the axes of maximum variance of the data. By projecting the data onto the axis of maximum variance, which form a lower-dimensional subspace, data redundancy is eliminated.

\vspace{1cm}

Finding the directions of maximum variance, which are referred to as principal components, is the central operation of \acrshort{pca}. One feasible option is to diagonalize the covariance matrix of the data. Recalling that each covariance matrix is a square symmetric matrix whose off-diagonal elements represent the covariances, i.e. reflect the redundancies, a diagonalization of the covariance matrix eliminates the magnitude of the covariance, resulting in a matrix that only contains the variance terms. Finally, each successive dimension in the diagonalized covariance matrix should be rank-ordered according to the values of the variance in order to quantify their importance.

It can be shown that the principal components are the eigenvectors of the covariance matrix of the data. A frequently used method for solving \acrshort{pca} is therefore the eigendecomposition, which is mathematically derived in the following in connection with the assumptions made so far.

\end{document}