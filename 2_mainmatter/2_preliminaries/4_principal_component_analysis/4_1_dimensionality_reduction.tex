\documentclass[../../../main.tex]{subfiles}
\begin{document}

\subsection{Dimensionality Reduction}\label{ch:dimensionality_reduction}

% This chapter focuses on dimensionality reduction of data in the context of Principal Component Analysis. First, the application of dimensionality reduction in general is motivated by various examples. Then, the core idea based on the separation of signal and noise in data is presented. After this, a concrete statement of the problem is introduced. In addition, key model assumptions and a solution approach is presented and followed by a mathematical derivation of the eigencomposition for solving \gls{pca}. Finally, a particular example is given.

% \subsubsection{Motivation}

% Several difficulties arise when analyzing high-dimensional data \cite[p.286]{dei_2020}. For example, it is often not easy to interpret which of the many factors have the most significant influence on the data distribution. In addition, data visualizations are often limited to a few dimensions. And besides the impact on data analysis, there are also influences on data inference. 

% Some machine learning algorithms cannot effectively reason about the underlying structure of the data in high-dimensional space, which can result into a model overfitting. Furthermore, compression of data, in terms of dimensionality reduction, can lead to advantages in processing and persistence of data. Data storage can often come at a high cost and some algorithms do not scale well with increasing numbers of dimensions. For example, when using the \gls{ema} (see Chapter \ref{ch:Expectation_Maximization_Algorithm}) increasing dimensionality of the data is related to an increase in the probability of the occurrence of (nearly) singular matrices, which can result in a numerical breakdown of the model \cite[p.434]{bis_2006}. For these reasons, dimensionality reduction is often desired within a data preprocessing strategy.
\end{document}