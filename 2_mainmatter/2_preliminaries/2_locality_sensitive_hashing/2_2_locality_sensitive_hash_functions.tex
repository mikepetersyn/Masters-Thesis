\documentclass[../../../main.tex]{subfiles}
\begin{document}

\subsection{Locality-Sensitive Hash Functions} \label{subsec:locality-sensitive-hashes}

Introduced by Indyk and Motwani in 1998 \cite{indyk_approximate_1998} as an algorithm that solves the \gls{cnn}, \gls{lsh} has since been extensively researched and is now considered among the state of the art for approximate searches in high-dimensional spaces.\footnote{See \cite{nagarkar2021exploring} for an exhaustive survey of techniques that solve the \gls{nn}.} The basic idea of the approach is to partition the input data using a hash function that is sensitive to the location of the input within the metric space. This way, similar inputs collide with a higher probability than inputs that are far apart. Thus, \gls{lsh} exhibits fundamental differences to conventional hash functions.\footnote{Cryptographic and non-cryptograhpic hash functions are referred to conventional hashing.}

A hash function is a function that maps a large input set to a smaller target set. The elements of the input set are called \textit{messages} or \textit{keys} and may be of arbitrary different lengths. The elements of the target set are called \textit{digests} or \textit{hash values} and are of fixed size length. 

We define a conventional hash function as $h: \mathcal{M} \rightarrow \mathcal{B}$ where $\mathcal{M} \subset \mathcal{X}$ is the input set and $\mathcal{B}=\{0, 1\}^K$ with $K \in \mathbb{N}$ the target set of all bit sequences of fixed size $K$, with $K < D$. It follows that messages $\bm{m} \in \mathcal{M}$ can only be mapped to $2^K$ different binary strings $\bm{b} \in \mathcal{B}$. The probability that a given $\bm{m}$ hashes to $\bm{b} = h(\bm{m})$ is $\frac{1}{2^K}$. Typically, conventional hashing is used for the realization of, e.g., hash tables, data integrity checks, error correction methods or database indexes. Such applications require a randomly uniform distribution of messages on hashes. Thus, a conventional hash function should therefore ideally assign hash values uniformly distributed with probability $\frac{1}{2^K}$ for all $\bm{m}$ and all $\bm{b}$. A related property in this context is the probability of a \textit{collision}. A collision occurs when two different messages $\bm{m}_1 \neq \bm{m}_2$ are projected onto the same hash value $h(\bm{m}_1) = h(\bm{m}_2)$. 

For example, cryptographic hash functions are used in systems, where additional security mechanisms are necessary. Thus, different security requirements are defined for cryptograhpic hash functions \cite[349]{williamcryptography}. In particular, these hash functions are designed to be resistant against collisions. Applications that do not require the hash function to be resistant against adversaries, e.g., hash tables, caches or de-duplication, are usually implemented by using a non-cryptographic hash function that exhibits relaxed guarantees on the security properties in exchange for significant performance improvements. However, both cryptographic and non-cryptographic hash functions behave relatively similar with respect to the distribution of messages on hashes and the probability of a collision. In contrast, locality-sensitive hash functions behave more or less inversely as illustrated in Figure~\ref{fig:hashing_differences}. A conventional hashing function $h$ does not incorporate the similarity of messages into the calculation of the hash value. As a result, the messages are hashed evenly distributed into the slots of $T_1$. A locality-sensitive hashing function $h_{\text{LSH}}$ on the other hand, hashes more similar messages into the same slot or neighbouring slots of $T_2$ as the hashes reflect the similarity of the input messages. In this regard, a hash table is introduced as an array $T$ in which each element is a subset of messages that share a common hash value. Additionally, the position, i.e., slot of each subset in $T$ is determined by that common hash value. For simplicity, we refer to a less mathematically sound notation within an algorithm description. Thus, the insert operation is denoted by $T[\bm{b}] = \bm{m}$, expressing that $\bm{m}$ is stored at slot $\bm{b} = h(\bm{m})$ within $T$. Conversely, the lookup operation is defined as $\mathcal{M}_{\bm{b}} = T[\bm{b}]$ for retrieving the subset of messages associated with the key $\bm{b}$.
\begin{figure}[t!]
    \centering
    \includestandalone{2_mainmatter/2_preliminaries/2_locality_sensitive_hashing/tikz/hashing_differences}
    \caption[Behaviour of hashing functions]{The behaviour of a conventional hashing function $h$ and a locality-sensitive hashing function $h_{\text{LSH}}$ when hashing very similar messages $\bm{m} \approx \bm{m}$, similar messages $\bm{m} \sim \bm{m}$ and non-similar messages $\bm{m} \not\sim \bm{m}$ into the slots of a hash table $T_1$ and $T_2$ respectively.}
    \label{fig:hashing_differences}
\end{figure}

As a locality-sensitive hashing function can be constructed as a general concept, specific families of functions can be derived. We refer to a \textit{family} of hash functions $\mathcal{H} = \{ h_1. \dots, h_K\}, h_k:\mathcal{M} \rightarrow \mathcal{B}$ as a collection of hash functions that have the same domain and range, share a basic structure and are only differentiated by constants. Three basic requirements are demanded for such a family of functions \cite[99]{leskovec_rajaraman_ullman_2014}.

\begin{enumerate}
    \item Close pairs of input should be hashed into the same bucket with higher probability than distant pairs.
    \item Functions within a family need to be statistically independent from one another, such that the FPR and FNR can be improved by combining two or more functions.
    \item Functions need to be efficient, i.e., be faster compared to an exhaustive search.

\end{enumerate}

The first step is to define LSH generally. Applied on the $cr$-ANN, the first requirement states, with high probability, two points $\bm{p}, \bm{q}$ should hash to the same hash value if their distance is at most $r$, i.e., $d(\bm{p},\bm{q}) \leq r$. And if their distance is at least $cr$, the points should hash to different hash values, i.e. $d(\bm{p},\bm{q}) > cr$. Thus,  A formal definition of a locality-sensitive hash function is given as follows \cite{andoni2006near}.

\begin{definition}[Locality-Sesitive Hash Function]
    Given a target distance $r \in \mathbb{R}, r>0$, an approximation factor $c \in \mathbb{R}, c>1$ and probability thresholds $p_1, p_2 \in \mathbb{R}$, where $p_1 > p_2$, a family $\mathcal{H} = \{h: \mathcal{P} \rightarrow \mathcal{B}$\} is called $(r, cr, p_1, p_2)$-sensitive if for any two points $\bm{p} \in \mathcal{P} \subset \mathcal{X},\bm{q} \in \mathcal{X}$ and any hash function $h$ chosen uniformly at random from $\mathcal{H}$ the following conditions are satisfied:
        \[
        \setlength\arraycolsep{0pt}
        \renewcommand\arraystretch{1.2}
            \begin{array}{LCL}
                d(\bm{p},\bm{q}) \leq r & \implies & P[h(\bm{p})=h(\bm{q})] \geq p_1, \\
                d(\bm{p},\bm{q}) \geq cr & \implies & P[h(\bm{p})=h(\bm{q})] \leq p_2.
            \end{array}
        \]
\end{definition}
 
Ideally, the gap between $p_1$ and $p_2$ should be as big as possible as depicted in Figure \ref{subfig:exact_probability}, which in fact represents an exact search. This is, as already discussed, no option due to its time and space requirements. Considering a single locality-sensitive function as shown in Figure \ref{subfig:lsh_probability}, where the probability gap between $p_1$ and $p_2$ is relatively close, the false negative rate would be relatively high. Increasing the gap would require to increase $c$ and lead to a high number of false positives. Therefore, a single function would provide only a tradeoff. But it is possible to increase $p_1$ close to $1$ and decrease $p_2$ close to $0$ while keeping $r$ and $cr$ fixed as shown in Figure \ref{subfig:lsh_probability_boosted} by introducing a process called \textit{amplification}. By applying a logical AND-construction on $\mathcal{H}$ both thresholds $p_1$ and $p_2$ are reduced. For that, $K$ hash functions are sampled indepedently from $\mathcal{H}$ and each point $\bm{p}$ is hashed to a $K$-dimensional vector with a new constructed function $g \in \mathcal{H}^K$ as

\begin{equation}\label{eq:or_construction}
    g(p) = [h_1(\bm{p}), h_2(\bm{p}), \dots, h_K(\bm{p})].
\end{equation}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includestandalone{2_mainmatter/2_preliminaries/2_locality_sensitive_hashing/tikz/lsh_probability}
        \caption{Single LSH Function.}
        \label{subfig:lsh_probability}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includestandalone{2_mainmatter/2_preliminaries/2_locality_sensitive_hashing/tikz/lsh_probability_boosted}
        \caption{Amplified LSH Function.}
        \label{subfig:lsh_probability_boosted}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \includestandalone{2_mainmatter/2_preliminaries/2_locality_sensitive_hashing/tikz/exact_probability}
            \caption{Exact Search}
            \label{subfig:exact_probability}
    \end{subfigure}
    \caption[The behaviour of a $(r, cr, p_1, p_2)$-sensitive function]{The behaviour of a $(r, cr, p_1, p_2)$-sensitive function in (\ref{sub@subfig:lsh_probability}) and (\ref{sub@subfig:lsh_probability_boosted}) (adapted from \cite[100]{leskovec_rajaraman_ullman_2014}) approaching the ideal probability gap in (\ref{sub@subfig:exact_probability}) resembling the behaviour of an exact search.}
    \label{fig:lsh_probability}
\end{figure}
   
Since all hash functions $\{h_1, \dots, h_K\} \in \mathcal{H}$ are statistically independent the product rule applies and for any two points $\bm{p}$ and $\bm{q}$, a collision occurs if and only if $h_k(\bm{p})=h_k(\bm{q})$ for all $k \in \{1, \dots, K\}$. The probability gap is then defined as follows:

\begin{align*}
    P[h_k(\bm{p})=h_k(\bm{q})] \geq p_1 \implies P[g(\bm{p})=g(\bm{q})] \geq p_1^K \\
    P[h_k(\bm{p})=h_k(\bm{q})] \leq p_2 \implies P[g(\bm{p})=g(\bm{q})] \leq p_2^K.
\end{align*}

Thus, by increasing $K$ the thresholds can be arbitrarily decreased. In order to increase both thresholds, the OR-construction is introduced. For that, a number of $L \in \mathbb{N}$ functions $g_1, \dots, g_L$ are constructed, where each function $g_l, \; l \in \{1, \dots, L\}$ stems from a different family $\mathcal{H}_l$. Note, that the algorithm is successful, when the two points $\bm{p}, \bm{q}$ collide at least once for some $g_l$. For $d(\bm{p}, \bm{q}) \leq r$, the following probability for a collision is given by

\begin{align*}
    P[\exists l \, | \, g_l(\bm{p})=g_l(\bm{q})] &= 1 - P[\forall l \,|\, g_l(\bm{p}) \neq g_l(\bm{q})] \\
                                &= 1 - P[g_l(\bm{p}) \neq g_l(\bm{q})]^L \\
                                &\geq 1 - (1-p_1^K)^L.
\end{align*}

As the AND-construction lowers both $p_1$ and $p_2$, similarly the OR-construction rises both thresholds. By choosing $K$ and $L$ judiciously, $p_2$ approaches zero while $p_1$ approaches one. Both constructions may be concatenated in any order to manipulate $p_1$ and $p_2$. Of course, the more constructions are used and the higher the values for the parameters $K$ and $L$ are picked, the more time is considered for the application of the function. The algorithm for solving the \gls{crnn} using \gls{lsh} consists of two consecutive steps, namely the preprocessing (Algorithm~\ref{alg:lsh_preprocess}) and the query (Algorithm~\ref{alg:lsh_query}).

\begin{algorithm}
    \caption[LSH preprocessing]{LSH preprocessing.}
    \label{alg:lsh_preprocess}
    % \algsetup{indent=2em}
    \begin{algorithmic}[1]
        \REQUIRE $N$ points $\bm{p}_n \in \mathcal{P}$ with $n \in \{1, \dots, N\}, N \in \mathbb{N}$
        \ENSURE data structure $\mathcal{T} = \{T_1, \dots, T_L\}$
        \STATE $\mathcal{T} \leftarrow \emptyset$
        \STATE construct hash functions $g_1, \dots, g_L$ each of length $K$ (see Equation \ref{eq:or_construction})
        \FORALL{$g_l \in \{g_1, \dots, g_L\}$}
            \STATE $T_l \leftarrow$ new hash table
            \FORALL{$\bm{p}_n \in \mathcal{P}$}
                \STATE $T_l[g_l(\bm{p}_n)] \leftarrow \bm{p}_n$
            \ENDFOR
            \STATE add $T_l$ to $\mathcal{T}$
        \ENDFOR
        \STATE \textbf{return} $\mathcal{T}$
    \end{algorithmic}
 \end{algorithm}

 First, a set of points $\mathcal{P}$ is required for the construction of the data structure $\mathcal{T}$ as described in Algorithm \ref{alg:lsh_preprocess}. For each function $g_l$ a corresponding hash table $T_l$ is initialized that will store all data points $\bm{p}_n$.  In other words, all data points are preprocessed and stored $L$ times.The resulting data structure represents the database, which is searched through, when answering a query. Note, that a common collision handling method, such as seperate chaining for example, is applied if a certain slot is already occupied.

 \begin{algorithm}
    \caption[LSH query]{LSH query}
    \label{alg:lsh_query}
    % \algsetup{indent=2em}
    \begin{algorithmic}[1]
        \REQUIRE a query point $\bm{q}$ and data structure $\mathcal{T} = \{T_1, \dots, T_L\}$
        \ENSURE a set $\mathcal{S}$ of nearest neighours of $\bm{q}$
        \STATE $\mathcal{S} \leftarrow \emptyset$
        \FORALL{$g_l \in \{g_1, \dots, g_L\}$}
            \STATE $\mathcal{S}_l \leftarrow T_l[g_l(\bm{q})]$
            \IF{$|\mathcal{S}_l| > 0$}
                \FORALL{$\bm{p}_n \in \mathcal{S}_l$}
                    \IF{$d(\bm{p}_n,\bm{q}) \leq cr$}
                        \STATE add $\bm{p}_n$ to $\mathcal{S}$
                    \ENDIF
                \ENDFOR
            \ENDIF
        \ENDFOR
        \STATE \textbf{return} $\mathcal{S}$
    \end{algorithmic}
 \end{algorithm}

 Answering a query point $\bm{q}$ is done by hashing it multiple times for each $g_l$ as in the preprocessing step (see Algorithm \ref{alg:lsh_query}). Each time, the set of points $\mathcal{P}$, stored in the correspoding hash table $T_l$ at the slot $g_l(\bm{q})$, are retrieved. That is, identify all $\bm{p}_n$, such that $g_l(\bm{p}_n) = g_l(\bm{q})$. For each identified $\bm{p}_n$, a distance function evaluates if the distance $d(\bm{p}_n, \bm{q})$ is within the search perimeter $cr$. If positive, the point is added to the result set $\mathcal{S}$, which is returned at the end of the algorithm.

 With regard to preprocessing, the consideration of space complexity of Algorithm \ref{alg:lsh_preprocess} is interesting, whereas the runtime of Algorithm \ref{alg:lsh_query} is the most relevant when examining the execution of queries. Fortunately, lower bounds on both quantities have been proven in \cite{motwani2006lower}, from which the optimization of the parameters $K$ and $L$ can be derived. Without giving details on the derivation of the results, the algorithm complexity is as follows. By ignoring lower order terms, the algorithm exhibits a query time of $O(N^\rho)$ and a space complexity of $O(N^{1+\rho})$. Thus, the query time and space consumption improves significantly as $\rho$ is decreased. The parameter $\rho$ is introduced as the main parameter that determines the running time and space consumption of the algorithm.


\end{document}
