\subsection{Collaborative Intrusion Detection}\label{subsec:cids}
With the commercialization of cloud products, various aspects related to the security of computer networks have also become more stringent. Not only has part of the responsibility for securing servers and networks shifted from the end customer to the service provider. The fact that computing resources within large data centers that are operated by public cloud providers are publicly accessible offers, on the one hand, a large contiguous attack surface. And on the other hand, a large amount of potential resources for executing so-called large-scale coordinated attacks are accessible to attackers. 

Typically, large-scale coordinated attacks target a large number of hosts which are spread over a wide geographical area \cite{Zhou2010}.
There, attackers make use of automation and sophisticated tools to target all vulnerable services at once instead of manually targeting services \cite{savage2005}. In the context of cloud environments, attackers can hijack mismanaged user accounts and hack into poorly secured cloud deployments \cite{kumar2019}. In this way, attackers can gain access to a variety of different resources to spread attacks across as many sources as possible, thus avoiding detection by security systems.

According to \cite{Zhou2010}, large-scale coordinated attacks are difficult to detect by isolated \gls{ids}, because of their limited monitoring scope. For instance, in \cite{riquet2012large} the authors show that a distributed portscan can be executed with relatively few resources without being detected when deploying Snort \gls{ids} and a commercial firewall solution. The experimental results show that the detection by the \gls{ids} can be bypassed most effectively by distributing the sources of a scan, whereas the detection by the firewall can be evaded by slowing down single executions of scans. 

In order to detect large-scale coordinated attacks, evidence of intrusions from multiple different sources need to be combined. This idea leads to the development of \gls{cids}, where the aggregation and correlation of data originating from different \glspl{ids} creates a holistic picture of the network to be monitored and enables the detection of distributed and coordinated attacks \cite[24]{vasilomanolakis_collaborative_2016}. \gls{cids} are also a solution to zero day attacks, because information about novel attacks can be shared with participants of the \gls{cids} in order to mitigate such attacks at an early stage. In addition to the aspect of improved detection, another advantage is the improved scaling for the protection of large IT systems. \glspl{cids} can monitor large-scale networks more effectively with the realization of a loadbalancing strategy. By sharing \gls{ids} resources across different infrastructures peak loads can be served resulting in a reduced downtime of individual \glspl{ids}.

 In general, a \gls{cids} is defined as a network of several intrusion detection components that collect and exchange data on system security. The tasks of a \gls{cids} can be allocated across two main components, namely the monitoring units and the analysis units. Monitoring units can be considered as conventional \gls{ids} that monitor a sub-network or a host and by that, generate low-level intrusion alerts. Analysis units are responsible for merging the low-level intrusion alerts and their further post-processing. This includes, for instance, the correlation of the alerts, the generation of reports or the distribution of the information to the participants of the network. The communication between the monitoring and analysis units is largely determined by the architecture of the \gls{cids}.

 \gls{cids} architectures can be categorized into centralized, hierarchical and decentralized approaches \cite{Zhou2010} (see Figure \ref{fig:cids_architectures}). Centralized architectures \cite{Cuppens2002}\cite{Miller2003}, consisting of multiple monitoring units and a central analysis unit, suffer from a conceptual \gls{spof} and are limited in their scalability due to a bottleneck, which is introduced by the central analysis unit. However, in practice the \gls{spof} can be avoided if individual components are implemented redundantly and form a centralized architecture only at the logical level. A bottleneck, on the other hand, is usually avoided by a distributed implementation of the logically centralized architecture. Hierarchical designs exhibit multiple monitoring and analysis units organized in a tree-based topology \cite{Phillip1997} \cite{Zhang2001} \cite{Nguyen2019}. These systems are restricted in their scalability by the respective instances on higher levels, whose failure results in a malfunction of the respective sub-trees \cite{Zhou2010}. 
 
 Again, such disadvantages only play a major role in non-redundantly implemented systems. Additionally, there exist approaches that are inherently distributed. Distributed architectures \cite{Janakiraman2003} \cite{Fung2008} \cite{Vasilomanolakis2015SkipMon}, wherein each participating system consists of both a monitoring and analysis unit and where the communication is based on some form of data distribution protocol, are considered as scalable by design. However, they depend on effective and consistent data dissemination and the data attribute selected for correlation may affect load distribution among participants~\cite{Zhou2010}. 

 \begin{figure}[t!]%
    \centering%
    \begin{subfigure}[b]{0.3\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/2_preliminaries/1_intrusion_detection/tikz/architecture_centralized}%
        \caption{}%
        \label{subfig:centralized}%
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.3\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/2_preliminaries/1_intrusion_detection/tikz/architecture_hierarchical}%
        \caption{}%
        \label{subfig:hierarchical}%
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.3\textwidth}%
        \centering\captionsetup{width=.8\linewidth}%
        \includestandalone{2_mainmatter/2_preliminaries/1_intrusion_detection/tikz/architecture_distributed}%
        \caption{}%
        \label{subfig:distributed}%
    \end{subfigure}%
    \caption[\gls{cids} architectures]{\gls{cids} architectures: (\subref{subfig:centralized}) centralized, (\subref{subfig:hierarchical}) hierarchical or (\subref{subfig:distributed}) distributed layout of monitoring (M) and aggregation units (A).}%
    \label{fig:cids_architectures}%
\end{figure}%

The authors in \cite{Vasilomanolakis2015SkipMon} describe the most important requirements that a \gls{cids} has to meet. Additionally, they further categorize the components of a \gls{cids} by function. The following is a summary of these requirements and components. Six requirements are defined, namely accuracy, minimal overhead and scalability, resilience, privacy, self-configuration and lastly interoperability. In this context, accuracy generally describes a collection of evaluation metrics for assessing the performance of the \gls{cids}. Frequently used metrics are, for example, the \gls{dr}, i.e. the ratio of correctly detected attacks to the total number of attacks, or the \gls{fpr}, which sets the number of normal data classified as an attack in relation to the total number of normal data. Obviously, a CIDS is expected to have higher accuracy in terms of attack detection than isolated IDSs. The requirement for minimal overhead and scalability addresses the operational overhead and the scalability of the system. First, the algorithms and techniques for collecting, correlating, and aggregating data must require minimal computational overhead. Second, data distribution within the \acrshort{cids} must be efficient. The performance required to operate the system should increase linearly with the resources used, so that computer systems of any size can be protected by the \gls{cids}. This property can be measured in theoretical terms by considering the complexity of the algorithms used. In practical terms, the passed time from the collection of a data point to the decision-making process can be measured. Resilience describes the ability of the system to be resistant to attacks, manipulations and system failures. A distinction is made between external attacks, such as DoS, and internal attacks by infiltrated \gls{cids} components. Moreover, it also covers fail-safety in general, which can be achieved, for example, by avoiding SPoFs. 

With regard to the protection and regulation of privacy in the \gls{cids}, a distinction is made between internal and external communication. Data that is exchanged internally in the \gls{cids} network between members may contain potentially sensitive information.

Such information should not be disclosed directly to other participants for reasons of data privacy. This includes, among other things, legal aspects when it comes to sharing log and network data. In connection with the resilience of the system, securing communication against third parties using cryptographic methods also plays a role and represents a major challenge, especially in dynamically distributed architectures. Self-configuration describes the degree of automation of a system with regard to configuration and operation. Particularly in distributed and complex architectures, a high degree of automation is desired in order to avoid operating errors and enable automatic resolution of component failures. Finally, individual components of the overall system, which were deployed in different system and network environments, should be able to interact with each other in the context of the \gls{cids}. In addition to system-wide standards for data collection, processing and exchange, there exists a trade-off between interoperability and privacy.

\begin{figure}[t]
    \centering
    \includestandalone{2_mainmatter/2_preliminaries/1_intrusion_detection/tikz/cids_components}
    \caption[\gls{cids} components]{Components of \gls{cids} according to functions.}
    \label{fig:cids_components}
\end{figure}

The domains into which the components of the \gls{cids} can be grouped are referred to as local monitoring, membership management, correlation and aggregation, data dissemination, and global monitoring. In the context of local monitoring, a distinction is made between active and passive monitoring. Active monitoring refers to the use of honeypots that reveal themselves as attack targets in the infrastructure in order to collect attack data. Passive monitoring involves intrusion detection activity at the host or network level, which in turn can be divided into misuse-based or anomaly-based methods according to the type of detection technique used (see Section~\ref{subsec:intrusion_detection_systems}). 

Membership management refers to ensuring secure communication channels in the form of an overlay network. Generally, membership management is categorized according to the organization and structure of the system topology. In terms of organization, there are either static approaches, in which members are added or removed manually, or dynamic approaches, in which components are organized automatically via a central entity or a protocol. Furthermore, the structure of the overlay network is relevant for the type of communication in the \gls{cids} network. This means that connections between the monitoring and analysis units are either centralized, hierarchical or distributed. 

Before collected and analyzed data are shared with other participants in the \gls{cids} network, the data is correlated and similar data points are aggregated. The main purpose of generating global and synthetic alerts is firstly to reduce the amount of alerts overall and secondly to improve data insights. Correlation mechanisms can be categorized into single-monitor and monitor-to-monitor approaches. 

Single-monitor methods correlate data locally without sharing data with other monitor entities. Monitor-to-monitor approaches, on the other hand, share data with other monitoring units in order to correlate local data with shared data and thus enable insights that go beyond isolated methods. Further, a classification can be made according to the correlation techniques used, grouping four different approaches.

Similarity-based approaches correlate data based on the similarity of one or more attributes. For instance, \cite{goo_2001} suggests using the 5-tuple information of the network data to detect duplicates within \glspl{nids}. The computation of similarity can be done in a variety of ways. For example, \cite{goo_2001b} defines a similarity function for each attribute and computes the overall data similarity by combining the functions using an expectation of similarity. High-dimensional data is usually problematic, as the difficulty for an effective calculation of similarity increases with the number of attributes \cite{zho_2009}.

Attack scenario-based approaches detect complex attacks based on databases that provide patterns for attack scenarios \cite{hut_2004} \cite{jaj_2002}. Such approaches have high accuracy for already known attacks. However, the accuracy decreases as soon as the patterns in the real data deviate from those in the database, since the definition of the scenario-rules are of a static quality in these approaches.

Multistage alert correlation aims to detect unknown multistage attacks by correlating defined pre- and post-conditions of individual alerts \cite{Cuppens2002} \cite{che_2003}. The idea behind the approach is based on the assumption that an attack is executed in preparation for a next step. The system states before and after an alert are modeled as pre- and postconditions, which are correlated with each other. While these approaches are more flexible than the all-static definition of attack scenarios, they still require the prior modeling of pre- and postconditions, which are based on expert knowledge.

Filter-based approaches filter irrelevant data in order to reduce the number of alerts within the intrusion detection context. For example, \cite{goo_2002} filters alerts by a ranking based on priorities of incidents. Priorities are calculated through comparing the topology familiar to the target with the vulnerability requirements of the incident type. The rank that each alert is assigned to provides the probability for its validity. The accuracy of the filters is based on the quality of the description of the topology to be protected and require reconfiguration when changes are made to the infrastructure.

Data Dissemination describes the efficient distribution of correlated and aggregated data in the \gls{cids} network. How the data is disseminated is strongly influenced by the topology and membership management of the system. Centralized topologies exhibit a prespecified information path from the monitoring units to the central analysis unit. If the system is organized hierarchically, information flows statically or dynamically organized from lower monitoring units in the hierarchy to higher units. Distributed topologies allow the use of versatile strategies, such as flooding, selective flooding or publish-subscribe methods.

Global monitoring mechanisms are needed for the detection of distributed attacks which isolated \glspl{ids} cannot detect due to the limited information base. Thus, the detection of distributed attacks relies on the insight obtained by combining data from different infrastructures. This means that global monitoring is strongly dependent on the employed correlation and aggregation techniques. The overall strategy of the \gls{cids} is described by global monitoring and can have both generic and specific objectives. 